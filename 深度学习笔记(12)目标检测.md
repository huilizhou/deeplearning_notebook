#### 目标定位
前面两节课，我们介绍的是利用CNN模型进行图像分类。现在我们将继续深入介绍目标定位和目标检测(包含多目标检测)。

![image](https://pic3.zhimg.com/80/v2-1bd91a5443ae0397b36f3d2d83478f8a_hd.jpg)

标准的CNN分类模型，我们已经很熟悉了，如下所示：

![image](https://pic3.zhimg.com/80/v2-b5195cb5d692762c831fa8ef22dfe965_hd.jpg)

原始图片经过卷积层后，Softmax层输出4x1的向量，分别是：

```math
\begin{bmatrix}1\\0\\0\\0 \end{bmatrix}
\begin{bmatrix}0\\1\\0\\0 \end{bmatrix}
\begin{bmatrix}0\\0\\1\\0 \end{bmatrix}
\begin{bmatrix}0\\0\\0\\1 \end{bmatrix}
```
注意Class label也可能是概率。上述四个向量分别对应pedestrain,car,motorcycle,background 四类。

对于目标定位和目标检测问题，其模型如下所示：

![image](https://pic1.zhimg.com/v2-96f4b812966166c89bd4f329c88d594b_r.jpg)

原始图片经过CONV卷积层后，Softmax层输出8x1的向量。除了包含一般CNN分类3x1向量(class label)之外，还包含(b<sub>x</sub>,b<sub>y</sub>)，表示目标中心位置坐标；还包含b<sub>h</sub>和b<sub>w</sub>，表示目标所在矩形区域的高和宽；还包含P<sub>c</sub>，表示矩形区域是目标的概率，数字在0~1之间，且越大概率越大。一般设定图片左上角为(0,0)，右下角为(1,1)。模型训练时，b<sub>x</sub>、b<sub>y</sub>、b<sub>h</sub>、b<sub>w</sub>都由人为的确定其数值。例如上图中，可得b<sub>x</sub>=0.5,b<sub>y</sub>=0.7,b<sub>h</sub>=0.3,b<sub>w</sub>=0.4。

输出label可表示为：

```math
\begin{bmatrix}P_c\\b_x\\b_y\\b_h\\b_w\\c_1\\c_2\\c_3 \end{bmatrix}
P_c=1:
\begin{bmatrix}1\\b_x\\b_y\\b_h\\b_w\\c_1\\c_2\\c_3 \end{bmatrix}
P_c=0:
\begin{bmatrix}0\\?\\?\\?\\?\\?\\?\\? \end{bmatrix}
```
若P<sub>c</sub>=0，表示没有检测到目标，则输出label后面的7个参数都可以忽略。

对于损失函数Loss function，若采用平方误差的形式有两种情况。
- P<sub>c</sub>=1，即y<sub>1</sub>=1:

```math
L(\hat y,y)=(\hat y_1-y_1)^2+(\hat y_2-y_2)^2+...+(\hat y_8-y_8)^2
```
- P<sub>c</sub>=0，即y<sub>1</sub>=0:

```math
L(\hat y,y)=(\hat y_1-y_1)^2
```
当然，除了使用平方误差之外，还可以把代数几率回归的损失函数，类标签c<sub>1</sub>,c<sub>2</sub>,c<sub>3</sub>也可以通过softmax输出。比较而言，平方误差已经能够取得比较好的结果。

---
#### 特征点检测
除了使用矩形区域检测目标类别和位置外，我们还可以仅对目标的关键特征点坐标进行定位，这些关键点被称为landmarks。

例如人脸识别，可以对人脸部分特征点坐标进行定位检测，并标记下来，如下图所示：

![image](https://pic4.zhimg.com/80/v2-8ea1a26106c34f8afd904faa2e040f38_hd.jpg)

该网络模型共检测人脸上64处特征点，加上是否为face的标志位，输出label共有64x2+1=129个值。通过检测人脸特征点可以进行情绪分类与判断，或者应用AR领域等等。

除了人脸特征点检测之外，还可以检测人体姿势动作，如下图所示：

![image](https://pic2.zhimg.com/80/v2-4446d8675c287dbe422c8aed55d04362_hd.jpg)


---
#### 目标检测
目标检测的一种简单方法是滑动窗口算法。这种算法首先在训练样本集上搜素相应的各种目标图片和非目标图片。注意训练集图片尺寸较小，尽量包含相应目标，如下图所示：

![image](https://pic3.zhimg.com/80/v2-f17f8d065ccdacc532a8cbec94c7e786_hd.jpg)

然后，使用这些训练集构建CNN模型，使得模型有较高的识别率。

最后，在测试图片上，选择大小合适的窗口、合适的步进长度，进行从左到右、从上到下的滑动。每个窗口区域都送入之前构建好的CNN模型进行识别判断。若判断有目标，则此窗口即为目标区域；若判断没有目标，则此窗口为非目标区域。

![image](https://pic2.zhimg.com/v2-0b76173d17954cf81d7d3f20b80cbf4a_r.jpg)

滑动窗算法的优点是原理简单，且不需要人为选定目标区域(检测出目标的滑动窗即为目标区域)。但是缺点也很明显，首先滑动窗的大小和步进长度都需要人为直观设定。滑动窗过小或过大，步进长度过大均会降低目标检测正确率。而且，每次滑动窗区域都要进行一次CNN网络计算，如果滑动窗和步进长度较小，整个目标检测的算法运行时间会很长。所有滑动窗算法虽然简单，但是性能不佳，不够快，不够灵活。

---
#### 卷积的滑动窗口的实现
滑动窗算法可以使用卷积的方式实现，以提高运行速度，节约重复运算成本。

首先，单个滑动窗口区域进入CNN网络模型时，包含全连接层。那么滑动窗口算法卷积实现的第一步就是将全连接层转化为卷积层，如下图所示：

![image](https://pic4.zhimg.com/80/v2-e07def84c54786b6c6c1157d16d6df37_hd.jpg)

全连接层转变成卷积层的操作很简单，只需要使用与上层尺寸一致的滤波算子进行卷积运算即可。最终得到的输出层维度是1x1x4，代表4类输出值。

输入图像是5x5x16，用5x5的过滤器对它进行卷积操作，过滤器实际上是5x5x16，因为卷积过程中，过滤器会遍历这16个通道，所以这两处的通道数量必须保持一致，输出的结果是1x1。假设应用400个这样5x5x16过滤器，输出维度就是1x1x400，我们不再把它看作一个含有400个节点的集合，而是一个1x1x400的输出层。从数学角度看，它和全连接层是一样的，因为这400个节点中每个节点都有一个5x5x16维度的过滤器，所以每个值都是上一层这些5x5x16激活值经过某个任意线性函数的输出结果。

<!--单个窗口区域卷积网络结构搭建完毕之后，对于待检测图片，CNN网络得到的输出层为8x8x4，共64个窗口结果。-->

假设向滑动窗口卷积网络输入 14×14×3 的图片，为了简化演示和计算过程，这里我们依然用14×14的小图片。和前面一样，神经网络最后的输出层，即softmax单元的输出是1×1×4，我画得比较简单，严格来说，14×14×3应该是一个长方体，第二个 10×10×16也是一个长方体，但为了方便，我只画了正面。所以，对于 1×1×400 的这个输出层，我也只画了它 1×1 的那一面，所以这里显示的都是平面图，而不是 3D 图像。 

![image](https://pic2.zhimg.com/80/v2-e59cda9164a22ebc791a54a1b8072c55_hd.jpg)

假设输入给卷积网络的图片大小是14x14x3，测试集图片是16x16x3，现在给这个输入图片加上黄色条块，在最初的滑动窗口算法中，你会把蓝色区域输入卷积网络生成0或1的分类。接着滑动窗口，步幅为2个像素，向右滑动2个像素，将这个区域输入给卷积网络，运行之后得到另外一个标签0或1。继续输入，最后对右下方的区域进行最后一次卷积操作。我们在16x16x3的小图像上滑动窗口，卷积网络运行了4次，于是输出4个标签。

结果发现这4次计算卷积有很多都是重复的。所以执行滑动窗口卷积时使得卷积网络在这4次前向传播过程中共享了很多参数，使得相同的5x5x16过滤器进行卷积操作，得到12x12x16的输出层，然后执行同样的最大池化，输出结果为6x6x16.照旧利用400个5x5的过滤器，得到一个2x2x400的输出层，而不是1x1x400。应用1x1的过滤器得到另一个2x2x400的输出层。再做一次全连接的操作，最终得到2x2x4的输出层，而不是1x1x4。


---
#### Bounding Box预测
滑动窗口算法有时会出现滑动窗不能完全涵盖目标的问题，如下图蓝色窗口所示。

![image](https://pic2.zhimg.com/80/v2-ec2cb1c564e0aefee39fca0fbf0bda40_hd.jpg)

YOLO(You Only Look Once)算法可以解决这类问题，生成更加准确的目标区域(如上图红色窗口)。

YOLO 算法首先将原始图片分割成nxn网格，每个网格代表一块区域。为简化说明，为简化说明，下图中将图片分成3x3网格。

![image](https://pic3.zhimg.com/80/v2-244fbef355b2d1eb46a768cc9a587b7b_hd.jpg)

然后，利用上一节卷积形式实现滑动窗口算法的思想，对该原始图片构建CNN网络，得到的输出层维度为3x3x8。其中，3x3对应9个网格，每个网格的输出包含8个元素：


```math
y=\begin{bmatrix}P_c\\b_x\\b_y\\b_h\\b_w\\c_1\\c_2\\c_3 \end{bmatrix}
```
如果目标中心坐标(b<sub>x</sub>,b<sub>y</sub>)不在当前网格内，则当前网格P<sub>c</sub>=0；相反，则当前网格P<sub>c</sub>=1(即只看中心坐标是否在当前网格内)。判断有目标的网格中，b<sub>x</sub>，b<sub>y</sub>,b<sub>h</sub>,b<sub>w</sub>限定了目标区域。值得注意的是，当前网格左上角坐标设定为(0,0)，右下角坐标设定为(1,1)，(b<sub>x</sub>,b<sub>y</sub>)范围限定在[0,1]之间，但是b<sub>h</sub>,b<sub>w</sub>可以大于1。因为目标可能超出该网格，横跨多个区域，目标占几个网格没有关系，目标中心坐标必然在一个网格之内。

划分的网格可以更密一些。网格越小，则多个目标的中心坐标被划分到一个网格内的概率就越小，这恰恰是我们想看到的。

---
#### 交并比
IoU(Intersection Over Union)，即交集与并集之比，可以用来评价目标检测区域的准确性。

![image](https://pic1.zhimg.com/v2-7ba2b8c4f32b6aa196badf6c0368eeca_r.jpg)

如上图所示，红色方框为真实目标检测区域，蓝色方框为检测目标区域。两块区域的交集为绿色部分，并集为紫色部分。蓝色方框和红色方框的接近程度可以用IoU比值来定义：

IoU
```math
IoU ={ I \over U}
```
IoU可以表示任意两块区域的接近程度。IoU值介于0~1之间，且越接近1表示两块区域越接近。

---
#### 非最大值抑制
YOLO算法中，可能会出现多个网络都检测到同一目标的情况，假如几个相邻网络都判断同一目标的中心坐标在其内。

![image](https://pic2.zhimg.com/80/v2-a5493574229fc7e1bfb84374cdd57874_hd.jpg)

上图中，三个绿色网格和三个红色网格分别检测的都是同一个目标。那如何判断哪个网格最为准确呢？方法是使用非最大值抑制法。

非最大值抑制法(Non-max Suppression)做法很简单，图示每个网络的P<sub>c</sub>值可以求出来，P<sub>c</sub>值反映了该网格包含目标中心坐标的可信度。首先选取P<sub>c</sub>最大值对应的网格和区域，然后计算该区域与所有其它区域的IoU，剔除掉IoU大于阈值(例如0.5)的所有网格及区域。这样就能保证同一个目标只有一个网格与之对应，且该网格P<sub>c</sub>最大，最可信。接着，再从剩下的网格中选取P<sub>c</sub>最大的网格，重复上一步的操作。最后，就能使得每个目标都仅由一个网格和区域对应。如下图所示：

![image](https://pic4.zhimg.com/80/v2-a1316e6a11fd562849730c97cd69888f_hd.jpg)

总结一下非最大值抑制算法的流程：
- 1.剔除P<sub>c</sub>值小于某阈值(例如0.6)的所有网络；
- 2.选取P<sub>c</sub>值最大的网络，利用IoU，摒弃与该网络交叠较大的网络；
- 3.对剩下的网络，重复步骤2。

---
#### Anchor Boxes
到目前为止，我们介绍的都是一个网络至多只能检测一个目标。那对于多个目标重叠的情况，例如一个人站在一辆车的前面，该如何使用YOLO算法进行检测呢？方法是使用不同形状的Anchor Boxes。

如下图所示，同一网络出现了两个目标：人和车。为了同时检测两个目标，我们可以设置两个Anchor Boxes,Anchor box1检测人，Anchor box2检测车。也就是说每个网络多加了一层输出，原来的输出维度是3x3x8，现在是3x3x2x8(也可以写成3x3x16的形式)。这里说的2表示有2两个Anchor Boxes，用来在一个网络中同时检测多个目标。每个Anchor box都有一个P<sub>c</sub>值，若两个P<sub>c</sub>值均大于某阈值，则检测到了两个目标。

![image](https://pic4.zhimg.com/80/v2-9d32c94040c61abd74bff78fc514fca3_hd.jpg)


```math
y = \begin{bmatrix}P_c\\b_x\\b_y\\b_h\\b_w\\c_1\\c_2\\c_3\\P_c\\b_x\\b_y\\b_h\\b_w\\c_1\\c_2\\c_3
\end{bmatrix}
```
在使用YOLO算法时，只需要对每个Anchor box使用上一节的非最大值抑制即可。Anchor Boxes 之间并行实现。

顺便提一下，Anchor Boxes形状的选择可以通过人为选取，也可以使用其他机器学习算法，例如k聚类算法对待检测的所有目标进行形状分类，选择主要形状作为Anchor Boxes。


---
#### YOLO 算法
这一节主要介绍YOLO算法，算是对前面几节内容的回顾。网络结构如下图所示，包含了两个Anchor Boxes。

- 1. For each grid cell, get 2 predicted bounding boxes.
- 2. Get rid of low probability predicitons.
- 3. For each class(pedestrian,car,motorcycle) use non-max suppression to generate final predictions.

![image](https://pic1.zhimg.com/v2-7482960b8a25e006da302b7750165d23_r.jpg)


---
#### RPN网络
之前介绍的滑动窗算法会对原始图片的每个区域都进行扫描，即使是一些空白的或明显没有目标的区域，例如下图所示。这样会降低算法运行效率，耗费时间。

![image](https://pic3.zhimg.com/80/v2-a0ffd445ea5a2b41e1ffec3b348afdbf_hd.jpg)

为了解决这一问题，尽量避免对无用区域进行扫描，可以使用Region Proposals 的方法。具体做法是先对原始图片进行分割算法处理，然后只对分割后的图片中的块进行目标检测。

![image](https://pic2.zhimg.com/v2-b4828749d8f39e299771f32de01fc3ce_r.jpg)

Region Proposals 共有三种方法：
- R-CNN：滑动窗的形式，一次只对单个区域块进行目标检测，运算速度慢。
- Fast R-CNN:利用卷积实现滑动窗算法。
- Faster R-CNN:利用卷积对图片进行分割，进一步提高运行速度。

相比较而言，Faster R-CNN的运行速度还是比YOLO慢一些。
