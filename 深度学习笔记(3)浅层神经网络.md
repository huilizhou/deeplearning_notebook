#### 神经网络概览
如何实现一个神经网络，前面我们讨论了对数几率回归，了解了其模型如何与下面的公式形成对应关系。

公式3.1：
```math
 \left.\begin{array}{l}x\\\omega\\b\end{array}\right\}\Rightarrow z=\omega^Tx+b

```
如上所示，首先你需要输入特征x，参数&omega;和b。通过这些你就可以计算出z。

公式3.2：
```math
 \left.\begin{array}{l}x\\\omega\\b\end{array}\right\}\Rightarrow z=\omega^Tx+b\Rightarrow a=\sigma(z)
 
 \Rightarrow L(a,y)

```
接下来，使用z就可以计算出a。我们将符号

```math
\hat y \Rightarrow a=\sigma(z)
```
然后可以计算出loss function L(a,y)

神经网络看起来是如下的样子。你可以把许多sigmoid单元堆叠起来形成一个神经网络。它包含了之前所讲的计算的两个步骤。首先通过公式计算出值z,然后通过&sigma;(z)计算值a。

![image](https://pic2.zhimg.com/v2-81fa399614a8494204981edef5d8720e_r.jpg)

在这个神经网络中，对应的3个节点，首先先计算第一层网络中各个节点相关的数z<sup>[1]</sup>，接着计算a<sup>[1]</sup>，在计算下一层网络的同理：我们使用符号 <sup>[m]</sup> 表示第m层网络中相关的节点数，这些节点的集合被称为第m层网络。这样可以保证 <sup>[m]</sup>不会和我们之前用来表示单个的训练样本 <sup>(i)</sup>混淆。
公式如下：

公式3.3：

```math
\left.\begin{array}{l}x\\W^{[1]}\\b^{[1]}\end{array}\right\}\Rightarrow z^{[1]}=W^{[1]}x+b^{[1]}\Rightarrow a^{[1]}=\sigma(z^{[1]})
```
公式3.4：

```math
\left.\begin{array}{l}x\\dW^{[1]}\\db^{[1]}\end{array}\right\}\Leftarrow dz^{[1]}=d(W^{[1]}x+b^{[1]})\Leftarrow da^{[1]}=d\sigma(z^{[1]})
```
类似于对数几率回归，在计算后需要使用计算，接下来你需要使用另外一个线性方程对应的参数计算z<sup>[2]</sup>，计算a<sup>[2]</sup>。此时a<sup>[2]</sup>就是整个神经网络的最终的输出。

公式3.5：
```math
\left.\begin{array}{l}da^{[1]}=d\sigma(z^{[1]})\\dW^{[2]}\\db^{[2]}\end{array}\right\}\Leftarrow dz^{[2]}=d(W^{[2]}a^{[1]}+b^{[2]})\Leftarrow da^{[2]}=d\sigma(z^{[2]})

\Leftarrow dL(a^{[2]},y)
```
这其中有很多细节，其中有一点非常难以理解，即在对数几率回归中，通过直接计算z得到结果a。而这个神经网络过程中，我们反复计算z和a，计算a和z，最后得到了最终的输出 loss function。

应该记得代数几率回归中，有一些从后向前的计算用来计算导数da、dz。同样，在神经网络中我们也有从后向前的计算，看起来就像上面那样，最后会计算da<sup>[2]</sup>、dz<sup>[2]</sup>，计算出来之后，然后计算dW<sup>[2]</sup>、db<sup>[2]</sup>等。按公式3.4-3.5箭头那样，从右到左，反向计算。


---
#### 神经网络的表示
我们首先关注一个例子，本例中的神经网络只包含一个隐藏层，下面是一张神经网络的图片。

![image](https://pic4.zhimg.com/v2-6e2f4cb90fca2a734e9a04d89adced54_r.jpg)

我们有输入特征x<sub>1</sub>、x<sub>2</sub>、x<sub>3</sub>，它们被竖直地堆叠起来，这叫做神经网络的**输入层**。它包含了神经网络的输入；然后这里有另外一层，我们称之为**隐藏层**(图上的4个节点)；在本例中最后一层只有一个结点构成，而这个只有一个结点构成的点被称为**输出层**，它负责产生预测值。解释隐藏层的含义：在一个神经网络中，当你使用监督学习训练它的时候，训练集包含了输入x也包含了目标输出y，所以术语隐藏层的含义是在训练集中，这些中间结点的准确值，我们是不知道的，也就是说你看不见它们在训练集中应具有的值。你能看见输入的值，你也能看见输出的值，但是隐藏层中的东西，在训练集中你是无法看到的。所以这也解释了词语隐藏层，只是表示你无法在训练集中看到他们。 

现在我们再引入几个符号，就像我们之前用向量x表示输入特征。这里有个可替代的记号a<sup>[0]</sup>可以用来表示输入层的特征。a(activation)表示激活的意思，a表示激活的意思，它意味着网络中不同层的值会传递到它们后面的层中，输入层将x传递给隐藏层，所以我们将输入层的激活值称为a<sup>[0]</sup>；下一层即隐藏层也同样会产生一些激活值，那么我将其记作a<sup>[1]</sup>，所以具体地，这里的第一个单元或结点我们将其表示为a<sup>[1]</sup><sub>1</sub>，第二个结点的值我们记为a<sup>[1]</sup><sub>2</sub>以此类推。所以这里的是一个四维的向量如果写成Python代码，那么它是一个规模为 4x1的矩阵或一个大小为4的列向量，如下公式，它是四维的，因为在本例中，我们有四个结点或者单元，或者称为四个隐藏层单元； 

公式3.6：

```math
a^{[1]}=\begin{bmatrix}a_1^{[1]}\\a_2^{[1]}\\a_3^{[1]}\\a_4^{[1]} \end{bmatrix}
```
最后输出层将产生某个数值a，它只是一个单独的实数，取值为a<sup>[2]</sup>，在对数几率回归中我们只有一个输出层，所以我们没有用带方括号的上标。但是在神经网络中，我们将使用这种带上标的形式来明确地指出这些值来自于哪一层，有趣的是在约定俗成的符号传统中，在这里你所看到的这个例子，只能叫做一个两层的神经网络。原因是当我们计算网络的层数时，输入层是不算入总层数内，所以隐藏层是第一层，输出层是第二层。第二个惯例是我们将输入层称为第零层，所以在技术上，这仍然是一个三层的神经网络，因为这里有输入层、隐藏层，还有输出层。但是在传统的符号使用中，如果你阅读研究论文或者在这门课中，你会看到人们将这个神经网络称为一个两层的神经网络，因为我们不将输入层看作一个标准的层。 

最后，我们要看到的隐藏层和最后的输出层都是带有参数的，这里的隐藏层将拥有两个参数W和b，我将给它们加上上标 <sup>[1]</sup> (W<sup>[1]</sup>,b<sup>[1]</sup>)，表示这些参数是和第一层这个隐藏层有关系的。之后，我们在例子中可以看到W是一个4 * 3的矩阵，而b是一个4 * 1的向量，第一个数字4源自于我们有四个结点或隐藏层单元，然后数字3源自于这里有三个输入特征，我们之后会更加详细地讨论这些矩阵的维数。相似的输出层也是有一些与之关联的参数W<sup>[2]</sup>以及b<sup>[2]</sup>。从维数上来看，它们的规模分别是1 * 4和1 * 1。1 * 4为隐藏层有四个隐藏层单元而输出层只有一个单元，之后我们会对这些矩阵和向量的维度做出更加深入的解释，所以现在你已经知道一个两层的神经网络什么样的了，即它是一个只有一个隐藏层的神经网络。


---
#### 计算一个神经网络的输出

我们了解一下神经网络是如何计算出来的。

![image](https://pic4.zhimg.com/v2-58e4c91ba258ee1162c905cf6c8ff73a_r.jpg)

神经网络是如何计算的，从我们之前提及的对数几率回归开始，用圆圈表示神经网络的计算单元，对数几率回归有两个步骤，首先你 按步骤先计算出z，然后在第二步中你以sigmoid函数作为激活函数计算z(得出a)，一个神经网络只是这样子做了好多次重复计算。

回到两层的神经网络，我们从隐藏层的第一个神经元开始计算，如上图第一个最上面的箭头所指。从上图可以看出，输入与逻辑回归相似，这个神经元的计算与逻辑回归一样分为两步，小圆圈代表了计算的两个步骤。

第一步：

```math
z_1^{[1]}=\omega_1^{[1]T}x+b_1^{[1]}
```
第二步：通过激活函数计算

```math
a_1^{[1]}=\sigma(z_1^{[1]})
```
隐藏层的第二个以及后面两个神经元的计算过程一样，只是注意符号表示不同，最终分别得到a<sup>[1]</sup><sub>2</sub>,a<sup>[1]</sup><sub>3</sub>,a<sup>[1]</sup><sub>4</sub>。详细结果如下：

```math
z_1^{[1]}=\omega_1^{[1]T}x+b_1^{[1]} , a_1^{[1]}=\sigma(z_1^{[1]})

z_2^{[1]}=\omega_2^{[1]T}x+b_2^{[1]} , a_2^{[1]}=\sigma(z_2^{[1]})

z_3^{[1]}=\omega_3^{[1]T}x+b_3^{[1]} , a_3^{[1]}=\sigma(z_3^{[1]})

z_4^{[1]}=\omega_4^{[1]T}x+b_4^{[1]} , a_4^{[1]}=\sigma(z_4^{[1]})

```
**向量化计算** 如果你执行神经网络的程序，用for循环来做看起来很低效。所以接下来，我们需要做的就是把这四个等式向量化。向量化的过程是将神经网络中的一层神经元参数纵向堆积起来，例如将隐藏层中的&omega;纵向堆积起来变成一个(4,3)的矩阵，用符号W<sup>[1]</sup>表示。另一个看待这个的方法是我们有四个对数几率回归单元，且每一个单元都有相应的参数————向量&omega;，把这四个向量堆积在一起，你会得出这4 * 3 的矩阵。
详细过程见下：

公式3.10
```math
a^{[1]}=\begin{bmatrix}
a_1^{[1]}\\a_2^{[1]}\\a_3^{[1]}\\a_4^{[1]}
\end{bmatrix}=\sigma(z^{[1]})
```
公式3.11

```math
\begin{bmatrix}
z_1^{[1]}\\z_2^{[1]}\\z_3^{[1]}\\z_4^{[1]}
\end{bmatrix}=\overbrace{\begin{bmatrix}\cdots \omega_1^{[1]T}\cdots\\\cdots \omega_2^{[1]T}\cdots\\\cdots \omega_3^{[1]T}\cdots\\\cdots \omega_4^{[1]T}\cdots\\
\end{bmatrix}}^{W^{[1]}}*
\overbrace{\begin{bmatrix}x_1\\x_2\\x_3
\end{bmatrix}}^{input}+
\overbrace{\begin{bmatrix}b_1^{[1]}\\b_2^{[1]}\\b_3^{[1]}\\b_4^{[1]}
\end{bmatrix}}^{b^{[1]}}

```
对于神经网络的第一层，给予一个输入x，得到a<sup>[1]</sup>，x可以表示为a<sup>[0]</sup>，通过相似的衍生，你会发现，后一层的表示同样可以写成类似的形式。

![image](https://pic1.zhimg.com/v2-2bdf8458c990a96543dc59df33b5facb_r.jpg)

如上图左半部分所示为神经网络，把网络左边部分盖住先忽略，那么最后的输出单元就相当于一个逻辑回归的计算单元。当你有一个包含一层隐藏层的神经网络，你需要去实现以计算得到输出的是右边的四个等式，并且可以看成是一个向量化的计算过程，计算出隐藏层的四个逻辑回归单元和整个隐藏层的输出结果，如果编程实现需要的也只是这四行代码。 

**总结**：你能够根据给出的一个单独的输入特征向量，运用四行代码计算出一个简单的神经网络的输出。接下来。我们要了解的是如何一次能够计算出不止一个样本的神经网络输出，而是能一次计算整个训练集的输出。

---
#### 多样本向量化
上个视频，我们了解到如何针对单一的训练样本，在神经网络上计算出预测值。

对数几率回归是将各个训练样本组合成矩阵，对矩阵的各列进行计算神经网络是通过对对数几率回归的等式简单的变形，让神经网络计算出输出值。这种计算是所有的训练样本同时进行的。

上一节得到的四个等式。它们给出了如何计算z<sup>[1]</sup>,a<sup>[1]</sup>,z<sup>[2]</sup>,a<sup>[2]</sup>。

对于一个给定的输入特征向量X，这是针对于单一的训练样本，如果有m个训练样本，那么就需要重复实现这一过程。

用第一个样本x<sup>(1)</sup>来计算预测值，就是第一个训练样本上得出的结果。然后用x<sup>(2)</sup>来计算第二个训练样本得出的结果。循环往复，直至用x<sup>(m)</sup>计算出第m个训练样本的结果。

用激活函数表示法，它写成a<sup>[2]</sup><sup>(1)</sup>、a<sup>[2]</sup><sup>(2)</sup>、a<sup>[2]</sup><sup>(m)</sup>。

注：a<sup>[2]</sup><sup>(i)</sup>,(i)是指第i个训练样本，而[2]是指第2层。

如果有一个非向量化形式的实现，而且要计算出它的预测值，对于所有训练样本，需要让i从1到m实现这四个等式：

```math
z^{[1](i)}=W^{[1](i)}x^{(i)}+b^{[1](i)}

a^{[1](i)}=\sigma(z^{[1](i)})

z^{[2](i)}=W^{[2](i)}a^{[1](i)}+b^{[2](i)}

a^{[2](i)}=\sigma(z^{[2](i)})
```
对于上面的这个方程中的 <sup>(i)</sup>，是所有依赖于训练样本的变量，即将(i)添加到x,z和a。如果想计算m个训练样本上的所有输出，就应该向量化整个计算，以简化这一列。

本课程需要使用很多线性代数的内容，重要的是能够正确地实现这一点，尤其是在深度学习的错误中。实际上本课程认真地选择了运算符号，这些符号只是针对于这个课程的，并且能使这些向量化容易一些。 

如何向量化这些：

公式3.12：
```math
x=\begin{bmatrix}\vdots&&\vdots&& &&\vdots\\
x^{(1)}&&x^{(2)}&&\cdots&&x^{(m)}\\\vdots&&\vdots&& &&\vdots
\end{bmatrix}
```
公式3.13：

```math
Z^{[1]}=\begin{bmatrix}\vdots&&\vdots&& &&\vdots\\
z^{[1](1)}&&z^{[1](2)}&&\cdots&&z^{[1](m)}\\\vdots&&\vdots&& &&\vdots
\end{bmatrix}
```
公式3.14：

```math
A^{[1]}=\begin{bmatrix}\vdots&&\vdots&& &&\vdots\\
a^{[1](1)}&&a^{[1](2)}&&\cdots&&a^{[1](m)}\\\vdots&&\vdots&& &&\vdots
\end{bmatrix}
```
公式3.15：

```math
\left.\begin{array}{l}z^{[1](i)}=W^{[1](i)}x^{(i)}+b^{[1]}\\a^{[1](i)}=\sigma(z^{[1](i)})\\z^{[2](i)}=W^{[2](i)}a^{[1](i)}+b^{[2]}\\a^{[2](i)}=\sigma(z^{[2](i)})\end{array}\right\}\Rightarrow \lbrace\begin{cases} A^{[1]}=\sigma(z^{[1]})\\z^{[2]}=W^{[2]}A^{[1]}+b^{[2]}\\A^{[2]}=\sigma(z^{[2]})\end{cases}
```
for循环式来遍历所有个训练样本。定义矩阵X等于训练样本，将它们组合成矩阵的各列，形成一个n维或n×m维矩阵。

依次类推，从小写的向量x到大写的矩阵X，只是通过组合x向量在矩阵的各列中，同理z<sup>[1]</sup><sup>(1)</sup>、z<sup>[1]</sup><sup>(2)</sup>...z<sup>[1]</sup><sup>(m)</sup>都是Z<sup>[1]</sup>的列向量。

同理a<sup>[1]</sup><sup>(1)</sup>、a<sup>[1]</sup><sup>(2)</sup>...a<sup>[1]</sup><sup>(m)</sup>将其组合到矩阵各列中，如同从向量x到矩阵X，以及从向量z到矩阵Z一样，就能得到矩阵A<sup>[1]</sup>。

同样的，对于Z<sup>[2]</sup>和A<suup>[2]</sup>，也是这样得到的。

这种符号的其中一个作用就是，可以通过训练样本来进行索引。这就是水平索引对应于不同的训练样本的原因，这些训练样本这些训练样本是从左到右扫描训练集而得到的。

在垂直方向，这个垂直索引对应于神经网络中不同的节点。例如这个节点位于矩阵的最左上角对应的激活单元，它是位于第一个训练样本上的第一个隐藏单元。它的下一个值对应于第二个隐藏单元的激活值。

当垂直扫描，是索引到隐藏单位的数字。当水平扫描，将从第一个训练示例中从第一个隐藏的单元到第二个训练样本，第三个训练样本……直到节点对应于第一个隐藏单元的激活值，且这个隐藏单元是位于这m个训练样本中的最终训练样本。 

从水平上看，矩阵𝐴代表了各个训练样本。从竖直上看，矩阵𝐴的不同的索引对应于不同的隐藏单元。 对于Z,X情况也类似。水平方向上，对应于不同的训练样本；竖直方向上，对应不同的输入特征，而这就是神经网络输入层中各个节点。

神经网络通过多样本情况下的向量化来使用这些等式。

---
#### 向量化实现的解释
我们先动手对几个样本计算一下前向传播

公式3.16：
```math
z^{[1](1)}=W^{[1]}x^{(1)}+b^{[1]}

z^{[1](2)}=W^{[1]}x^{(2)}+b^{[1]}

z^{[1](3)}=W^{[1]}x^{(3)}+b^{[1]}

```
这里为了叙述的简便，先忽略到b<sup>[1]</sup>。后面利用Python中的广播机制可以很容易的将b<sup>[1]</sup>加进来。

现在W<sup>[1]</sup>是一个矩阵，x<sup>(1)</sup>,x<sup>(2)</sup>,x<sup>(3)</sup>都是列向量，矩阵乘以列向量得到列向量，下面将它们用图形直观的表示出来：

公式3.17：

```math
W^{[1]}x=\begin{bmatrix}
\cdots\\\cdots\\\cdots
\end{bmatrix}*
\begin{bmatrix}
\vdots&&\vdots&&\vdots&&\vdots\\
\omega^{(1)}x^{(1)}&&\omega^{(1)}x^{(2)}&&\omega^{(1)}x^{(3)}&&\vdots\\
\vdots&&\vdots&&\vdots&&\vdots
\end{bmatrix}=
\begin{bmatrix}
\vdots&&\vdots&&\vdots&&\vdots\\
z^{[1](1)}&&z^{[1](2)}&&z^{[1](3)}&&\vdots\\
\vdots&&\vdots&&\vdots&&\vdots
\end{bmatrix}=
Z^{[1]}
```
所以，从这里我们也可以了解到，因为当有不同的训练样本时，把它们堆到矩阵X的各列中，那么它们的输出也就会相应的堆叠到矩阵Z<sup>[1]</sup>的各列中，现在我们就可以直接计算矩阵Z<sup>[1]</sup>加上b<sup>[1]</sup>，因为列向量b<sup>[1]</sup>和矩阵Z<sup>[1]</sup>的列向量有着相同的尺寸，而Python的广播机制对于这种矩阵与向量直接相加的处理方式是，将向量与矩阵的每一列相加。这一节只是说明为什么公式Z<sup>[1]</sup>=W<sup>[1]</sup>X+b<sup>[1]</sup>是前向传播的第一步计算的正确向量化实现，但事实证明，类似的分析可以发现，前向传播的其他步也可以采用相似的逻辑，即如果将输入列向量横向堆叠进矩阵，那么通过公式计算之后，也能得到成列堆叠的输出。

由公式 3.12、公式 3.13、公式 3.14、公式 3.15 可以看出，使用向量化的方法，可以不需要显示循环，而直接通过矩阵运算从𝑋就可以计算出A<sup>[1]</sup>，实际上𝑋可以记为A<sup>[0]</sup>，使用同样的方法就可以由神经网络中的每一层的输入 A<sup>[i-1]</sup>]计算输出A<sup>[i]</sup>。其实这些方程有一定对称性，其中第一个方程也可以写成Z<sup>[1]</sup>=W<sup>[1]</sup>X+b<sup>[1]</sup>，你看这对方程，还有这对方程形式其实很类似，只不过这里所有指标加了1。所以这样就显示出神经网络的不同层次，你知道大概每一步做的都是一样的，或者只不过同样的计算不断重复而已。这里我们有一个双层神经网络，我们在下周视频里会讲深得多的神经网络，你看到随着网络的深度变大，基本上也还是重复这两步运算，只不过是比这里你看到的重复次数更多。

---
#### 激活函数
使用一个神经网络，需要决定使用哪种激活函数用隐藏层上，哪种用在输出节点上。

在神经网络的前向传播中，a<sup>[1]</sup>=&sigma;(z<sup>[1]</sup>)和a<sup>[2]</sup>=&sigma;(z<sup>[2]</sup>)这两步会使用到sigmoid函数。sigmoid函数在这里被称为激活函数。

sigmoid函数：
![image](https://pic4.zhimg.com/v2-a9f105c7ae4f17d6aa83117412f52ed1_r.jpg)

更通常的情况下，使用不同的函数𝑔(z<sup>[1</sup>])，𝑔可以是除了 sigmoid 函数意外的非线性函
数。tanh 函数或者双曲正切函数是总体上都优于 sigmoid 函数的激活函数。 

tanh函数：
![image](https://pic4.zhimg.com/v2-9ff54fdbfbdb21b7556e09dd91ac1d15_r.jpg)

事实上，tanh函数是sigmoid的向下平移和伸缩后的结果。对它进行了变形后，穿过了(0,0)点，并且值域介于+1 和-1 之间。  𝑔(𝑧<sup>[1]</sup>) = 𝑡𝑎𝑛ℎ(𝑧<sup>[1]</sup>) 效果总是优于sigmoid 函数。因为函数值域在-1和+1的激活函数，其均值是更接近零均值的。在训练一个算法模型时，如果使用 tanh 函数代替sigmoid函数中心化数据，使得数据的平均值更接近 0 而不是 0.5。

在讨论优化算法时，有一点要说明：我基本已经不用 sigmoid 激活函数了，tanh函数在所有场合都优于 sigmoid 函数。除了在二分类问题中，因为y的值是0或1，所以想让y的估计的数值介于0和1之间，而不是-1和+1之间，所以需要使用sigmoid函数。

在不同的神经网络层中，激活函数可以不同。为了表示不同的激活函数，在不同的层中，使用方括号上标来指出𝑔上标为[1]的激活函数，可能会跟𝑔上标为[2]不同。方括号上标[1]代表隐藏层，方括号上标[2]表示输出层。

sigmoid 函数和tanh函数两者共同的缺点是，在𝑧特别大或者特别小的情况下，导数的梯度或者函数的斜率会变得特别小，最后就会接近于0，导致降低梯度下降的速度。 

只要𝑧是正值的情况下，导数恒等于1，当𝑧是负值的时候，导数恒等于0。从实际上来说，当使用𝑧的导数时，𝑧=0的导数是没有定义的。但是当编程实现的时候，𝑧的取值刚好等于0.0000000，这个值相当小，所以，在实践中，不需要担心这个值，𝑧是等于 0 的时候，假设一个导数是1或者0效果都可以。

ReLU函数：
![image](https://pic2.zhimg.com/v2-dade0aa81457d9f5b9bd345bcc3b41bd_r.jpg)

这有一些选择激活函数的经验法则： 如果输出是 0、1 值（二分类问题），则输出层选择 sigmoid 函数，然后其它的所有单元都选择 Relu 函数。 这是很多激活函数的默认选择，如果在隐藏层上不确定使用哪个激活函数，那么通常会使用 Relu 激活函数。有时，也会使用 tanh 激活函数，但 Relu 的一个优点是：当𝑧是负值的时候，导数等于 0。 这里也有另一个版本的 Relu 被称为 Leaky Relu。 当𝑧是负值时，这个函数的值不是等于 0，而是轻微的倾斜，

Leaky ReLU函数：
![image](https://pic3.zhimg.com/v2-65f06248d03688d2b0f557a4fe9bfe3e_r.jpg)

这个函数通常比Relu激活函数效果要好，尽管在实际中 Leaky ReLu 使用的并不多。 

两者的优点是：

第一，在𝑧的区间变动很大的情况下，激活函数的导数或者激活函数的斜率都会远大于0，在程序实现就是一个 if-else 语句，而sigmoid函数需要进行浮点四则运算，在实践中，使用 ReLu 激活函数神经网络通常会比使用 sigmoid 或者 tanh 激活函数学习的更快。

第二，sigmoid和tanh函数的导数在正负饱和区的梯度都会接近于 0，这会造成梯度弥散，而 Relu 和 Leaky ReLu 函数大于0部分都为常熟，不会产生梯度弥散现象。(同时应该注意到的是，Relu进入负半区的时候，梯度为0，神经元此时不会训练，产生所谓的稀疏性，而 Leaky ReLu 不会有这问题) 

快速概括一下不同激活函数的过程和结论。 

sigmoid 激活函数：除了输出层是一个二分类问题基本不会用它。

tanh 激活函数：tanh是非常优秀的，几乎适合所有场合。 

ReLu 激活函数：最常用的默认函数，，如果不确定用哪个激活函数，就使用 ReLu 或者Leaky ReLu。

如果不确定哪一个激活函数效果更好，可以把它们都试试，然后在验证集或者发展集上进行评价。然后看哪一种表现的更好，就去使用它。 

为自己的神经网络的应用测试这些不同的选择，会在以后检验自己的神经网络或者评估算法的时候，看到不同的效果。如果仅仅遵守使用默认的 ReLu 激活函数，而不要用其他的激励函数，那就可能在近期或者往后，每次解决问题的时候都使用相同的办法。 

---
#### 为什么需要非线性激活函数
事实证明要让你的神经网络能够计算出有趣的函数，你必须使用非线性激活函数。

如果我们去掉函数g，然后令a<sup>[1]</sup>=z<sup>[1]</sup>，或者我们也可以令g(z)=z，这个有时被叫做线性激活函数(更为学术点的名字是恒等激励函数，因为它们就是把输入值输出)。为了说明问题，我们把a<sup>[2]</sup>=z<sup>[2]</sup>，那么这个模型的输出仅仅只是输入特征x的线性组合。

如果你是用线性激活函数或者叫恒等激励函数，那么神经网络只是把输入线性组合再输出。

深度网络，有很多层的神经网络，很多隐藏层。事实证明，如果你使用线性激活函数或者没有使用一个激活函数，那么无论你的神经网络有多少层一直在做的只是计算线性函数，所以不如直接去掉全部隐藏层。在我们的简明案例中，事实证明如果你在隐藏层用线性激活函数，在输出层用 sigmoid 函数，那么这个模型的复杂度和没有任何隐藏层的标准 Logistic 回归是一样的。

在这里线性隐层一点用也没有，因为这两个线性函数的组合本身就是线性函数，所以除非你引入非线性，否则你无法计算更有趣的函数，即使你的网络层数再多也不行。只有一个地方可以使用线性激活函数————g(z)=z，就是你在做机器学习中的回归问题。y是一个实数。举个例子，比如你想预测房地产的价格，y就不是一个二分类任务0 或 1，而是一个实数。从0到正无穷。如果y是一个实数，那么在输出层用线性激活函数也许可行，你的输出也是一个实数，从负无穷到正无穷。

总而言之，不能再隐藏层使用线性激活函数，可以用ReLU或者tanh或者leaky ReLU 或其他的非线性激活函数，为一可用线性激活函数的通常就是输出层，；除了这种情况，会在隐层用线性函数的，除了一些特殊情况，比如与压缩有关的，那方面在这里将不深入讨论。

---
#### 激活函数的导数
在神经网络中使用反向传播的时候，你真的需要计算激活函数的斜率或者导数。针对以下四种激活函数，求其导数如下：

1. sigmoid函数
```math
g(z)={1 \over {1+e^{-z}}}

{d\over dz}g(z)={e^{-z}\over (1+e^{-z})^2}=
{1 \over {1+e^{-z}}}*(1-{1 \over {1+e^{-z}}})=g(z)(1-g(z))

```
在神经网络中

```math
a=g(z);

g(z)'={d\over dz}g(z)=a(1-a)

```
2. Tanh函数

```math
g(z)=tanh(z)={{e^z-e^{-z}}\over {e^z+e^{-z}}}

{d\over dz}g(z)={{(e^z+e^{-z})^2-(e^z-e^{-z})^2}\over {(e^z+e^{-z})^2}}=
1-(tanh(z))^2
```
3. ReLU函数

```math
g(z)=max(0,z)

g'(z)=\begin{cases}
        0,  & \text{if $z$ < 0} \\
        1, & \text{if $z$ > 0 } \\
        undefined,& \text{if $z$ = 0 } 
        \end{cases}
```
注意：通常在z=0的时候给定其导数1，当然z=0的情况很少。

4. Leaky ReLU函数

```math
g(z)=max(0.01z,z)

g'(z)=\begin{cases}
        0.01,  & \text{if $z$ < 0} \\
        1, & \text{if $z$ > 0 } \\
        undefined,& \text{if $z$ = 0 } 
        \end{cases}
```
与ReLU类似，通常在z=0的时候给定其导数1，当然z=0的情况很少。

---
#### 神经网络的梯度下降
你的单隐层神经网络会有W<sup>[1]</sup>,b<sup>[1]</sup>,W<sup>[2]</sup>,b<sup>[2]</sup>这些参数，还有n<sub>x</sub>表示输入特征的个数，n<sup>[1]</sup>表示隐藏层的个数，n<sup>[2]</sup>表示输出单元个数。

在我们的例子中，我们只介绍过这样的情况，那么参数

矩阵W<sup>[1]</sup>的维度就是(n<sup>[1]</sup>,n<sup>[0]</sup>),b<sup>[1]</sup>就是n<sup>[1]</sup>维向量，也可以写成(n<sup>[1]</sup>,1),就是一个列向量。矩阵W<sup>[2]</sup>的维度就是(n<sup>[2]</sup>,n<sup>[1]</sup>),b<sup>[2]</sup>的维度就是(n<sup>[2]</sup>,1)维度。

你还有一个神经网络的成本函数，假设你在做二分类任务，那么你的成本函数等于：

```math
J(W^{[1]},b^{[1]},W^{[2]},b^{[2]})={1\over m}\sum_{i=1}^mL(\hat y ,y)
```
loss function 和之前做的logistic 回归完全一样。

训练参数需要做梯度下降，在训练神经网络的时候，随机初始化参数很重要，而不是初始化成本全为零。当你参数初始化成某些值后。每次梯度下降都会循环计算以下预测值：

```math
\hat y^{(i)} ,(i=1,2,...,m)

dW^{[1]}={dJ\over dW^{[1]}},db^{[1]}={dJ\over db^{[1]}}

dW^{[2]}={dJ\over dW^{[2]}},db^{[2]}={dJ\over db^{[2]}}
```
其中

```math
W^{[1]}\Rightarrow W^{[1]}-\alpha dW^{[1]}, b^{[1]}\Rightarrow b^{[1]}-\alpha db^{[1]}

W^{[2]}\Rightarrow W^{[2]}-\alpha dW^{[2]}, b^{[2]}\Rightarrow b^{[2]}-\alpha db^{[2]}
```

正向传播方程：(forward propagation)

```math
Z^{[1]}=W^{[1]}X+b^{[1]}

A^{[1]}=g^{[1]}(Z^{[1]})

Z^{[2]}=W^{[2]}A^{[1]}+b^{[2]}

A^{[2]}=g^{[2]}(Z^{[2]})
```
其中，g(·)表示激活函数。

反向传播(back propagation)是计算导数（梯度)的过程：

```math
dZ^{[2]}=A^{[2]}-Y ,
Y=\begin{bmatrix}y^{(1)}&&y^{(2)}&&\cdots&&y^{(m)}\end{bmatrix}

dW^{[2]}={1\over m}dZ^{[2]}A^{[1]T}

db^{[2]}={1\over m} np.sum(dZ^{[2]},axis=1,keepdim=True)

dZ^{[1]}=W^{[2]T}dZ^{[2]}*g^{[1]'}(Z^{[1]})

dW^{[1]}={1\over m}dZ^{[1]}X^{T}

db^{[1]}={1\over m} np.sum(dZ^{[1]},axis=1,keepdim=True)

```
上述是反向传播的步骤，注：这些都是针对所有样本进行过向量化，𝑌是1 × 𝑚的矩阵；这里 np.sum 是 python 的 numpy 命令，axis=1 表示水平相加求和，keepdims 是防止python 输出那些古怪的秩数(𝑛,)，加上这个确保阵矩阵𝑑𝑏[2]这个向量输出的维度为(𝑛,1)这样标准的形式。


```math
dZ^{[1]}=\underbrace{W^{[2]T}dZ^{[2]}}_{(n^{[1]},m)}*\underbrace{g^{[1]'}}_{activaiton \, function\, of \,hidden\, layer}*\underbrace{(Z^{[1]})}_{(n^{[1]},m)}
```
你需要计算隐藏层的导数，输出在使用sigmoid函数进行二元分类。这里是进行逐个元素乘积。


---
#### 直观理解反向传播函数(选看)
![image](https://pic1.zhimg.com/v2-95a4f9fc0bfc48af25bdea11a468e8fa_r.jpg)

![image](https://pic4.zhimg.com/v2-881f7bdd13cb296b14f52d431b2c2ae3_r.jpg)

吴恩达老师认为反向传播的推导机制是机器学习领域最难的数学推导之一，矩阵的导数要用到链式法则求导。

---
#### 随机初始化
当你训练神经网络的时候，权重随机初始化时很重要的。对于对数几率回归，把权重初始化为0也是可以的。但是对于一个神经网络，如果你把权重或者参数都初始化为0，那么梯度下降就不会起作用了。

![image](https://pic4.zhimg.com/v2-87fe81fd7837e596ac709bdb7938242b_r.jpg)

举个简单的例子，一个浅层神经网络包含两个输入，隐藏层包含两个神经元。如果权重W<sup>[1]</sup>和W<sup>[2]</sup>都初始化为0，即：

```math
W^{[1]}= \left[ \begin{matrix} 0 & 0 \\ 0 & 0 \end{matrix} \right]



W^{[2]}= \left[ \begin{matrix} 0 & 0 \end{matrix} \right]
```

如果你把权重都初始化为0，那么由于隐含单元开始计算同一个函数，所有的隐含单元就会对输出单元有同样的影响。一次迭代后同样的表达式结果仍然是相同的，即隐含单元仍是对称的。通过推导，两次、三次、无论多少次迭代，不管你训练网络多长时间，隐含单元仍然计算的是同样的函数。因此这种情况下超过1个隐含单元也没什么意义，因为他们计算同样的东西。当然更大的网络，比如你有 3 个特征，还有相当多的隐含单元。

值得一提的是，参数b可以全部初始化为零，并不会影响神经网络训练效果。

我们把这种权重W全部初始化为零带来的问题称为symmetry breaking problem。解决方法也很简单，就是将W进行随机初始化（b可初始化为零）。python里可以使用如下语句进行W和b的初始化：

```
W_1 = np.random.randn((2,2))*0.01
b_1 = np.zero((2,1))
W_2 = np.random.randn((1,2))*0.01
b_2 = 0
```
你也许会疑惑，这个常数从哪里来，为什么是 0.01，而不是 100 或者 1000。我们通常倾向于初始化为很小的随机数。

如果𝑤很大，那么你很可能最终停在（甚至在训刚刚刚开始的时候）𝑧很大的值，这会造成 tanh/Sigmoid激活函数饱和在龟速的学习上，如果你没有 sigmoid/tanh 激活函数在你整个的神经网络里，就不成问题。但如果你做二分类并且你的输出单元是 Sigmoid函数，那么你不会想让初始参数太大，因此这就是为什么乘上 0.01 或者其他一些小数是合理的尝试。

事实上有时有比0.01更好的常数，当你训练一个只有一层隐藏层的网络时(这是相对浅的神经网络，没有太多的隐藏层)，设为0.01可能也可以。但当你训练一个非常非常深的神经网络，你可能会选择一个不同于的常数而不是 0.01。

