#### 深层神经网络
目前为止，在前3节中，我们学习了只有一个单独隐藏层的神经网络结构的正向传播和反向传播，还有对数几率回归，并且还学到了向量化，这在随机初始化权重的时候是很重要的。

![image](https://pic3.zhimg.com/v2-3d6d8d46821398f6f38b32fff994d0aa_r.jpg)

严格上来说对数几率回归也是一个一层的神经网络，而上边右图一个深得多的模型，浅与深仅仅是指一种程度。记住以下要点： 

有一个隐藏层的神经网络，就是一个两层神经网络。记住当我们算神经网络的层数时，我们不算输入层，我们只算隐藏层和输出层。 

但是在过去的几年中，DLI（深度学习学院 deep learning institute）已经意识到有一些函数，只有非常深的神经网络能学会，而更浅的模型则办不到。尽管对于任何给定的问题很难去提前预测到底需要多深的神经网络，所以先去尝试逻辑回归，尝试一层然后两层隐含层，然后把隐含层的数量看做是另一个可以自由选择大小的超参数，然后再保留交叉验证数据上评估，或者用你的开发集来评估。

我们再看下深度学校符号的定义：

![image](https://pic2.zhimg.com/v2-f766e253c42fc19cc9888029165762e1_r.jpg)

上面是一个四层神经网络，有三个隐藏层，我们可以看到，第一层(即左边数过去第二层，因为输入层是第0层)有5个神经元数目，第二层5个，第三层3个。

我们用L表示层数，上图：L=4，输入层的索引为“0”，第一个隐藏层n<sup>[1]</sup>=5，表示有5个隐藏神经元，同理n<sup>[2]</sup>=5,n<sup>[3]</sup>=3,n<sup>[4]</sup>=n<sup>[L]</sup>=1(输出单元为1)。而输入层，n<sup>[0]</sup>=n<sup>[x]</sup>=3。

在不同层所拥有的神经元的数目，对于每层l都用a<sup>[l]</sup>来记作l层激活后的结果。

通过激活函数g计算z<sup>[l]</sup>，激活函数也被索引为层数l，然后我们用w<sup>[l]</sup>来记作在l层计算z<sup>[l]</sup>值的权重，类似的b<sup>[l]</sup>也一样。

最后总结一下符号约定：

输入的特征记作x，但是x同样也是0层的激活函数，所以x=a<sup>[0]</sup>。a<sup>[l]</sup>是等于这个神经网络所预测的输出结果。

---
#### 深层网络中的前向传播
根往常一样，我们先来看对其中一个训练样本x如何应用前向传播，之后讨论向量化的版本。

仍以前面的四层网络为例；

第一层需要计算:z<sup>[1]</sup>=W<sup>[1]</sup>x+b<sup>[1]</sup>,            a<sup>[1]</sup>=g<sup>[1]</sup>(z<sup>[1]</sup>)

第二层需要计算:z<sup>[2]</sup>=W<sup>[2]</sup>a<sup>[1]</sup>+b<sup>[2]</sup>,
a<sup>[2]</sup>=g<sup>[2]</sup>(z<sup>[2]</sup>)

依次类推：

第四层为：z<sup>[4]</sup>=W<sup>[4]</sup>a<sup>[3]</sup>+b<sup>[4]</sup>,
a<sup>[4]</sup>=g<sup>[4]</sup>(z<sup>[4]</sup>)

前向传播可以归纳为多次迭代:
z<sup>[l]</sup>=W<sup>[l]</sup>a<sup>[l-1]</sup>+b<sup>[l]</sup>,
a<sup>[l]</sup>=g<sup>[l]</sup>(z<sup>[l]</sup>)

如果有m个样本，起向量化矩阵形式为：

```math
Z^{[l]}=W^{[l]}A^{[l-1]}+b^{[l]},
A^{[l]}=g^{[l]}(Z^{[l]})

(A^{[0]}=X)
```
这里只能用一个显示for循环，l从1到L，然后一层接着一层去计算。

---
#### 核对矩阵的维数
当实现深度神经网络的时候，其中一个我常用的检查代码是否有错的方法就是拿出一张纸过一遍算法中矩阵的维数。

&omega;的维度是(本层的维数，前一层的维数)，即&omega;<sup>[l]</sup>:(n<sup>[l]</sup>,n<sup>[l-1]</sup>);

b的维度是(本层的维数，1)，即b<sup>[l]</sup>:(n<sup>[l]</sup>,1);

z<sup>[l]</sup>,a<sup>[l]</sup>:(n<sup>[l]</sup>,1)

d&omega;<sup>[l]</sup>和&omega;<sup>[l]</sup>维度相同，db<sup>[l]</sup>和b<sup>[l]</sup>维度相同，且&omega;和b向量化维度不变。但z,a以及x的维度向量化后发生变化。

向量化后：
Z<sup>[l]</sup>可以看成由每个单独的z<sup>[l]</sup>叠加而成，Z<sup>[l]</sup>=(Z<sup>[l]</sup><sup>(1)</sup>,Z<sup>[l]</sup><sup>(2)</sup>,Z<sup>[l]</sup><sup>(3)</sup>,...,Z<sup>[l]</sup><sup>(m)</sup>)，m为训练集的大小，所以Z<sup>[l]</sup>的维度不再是(n<sup>[l]</sup>,1)，而是(n<sup>[l]</sup>,m)。

A<sup>[l]</sup>:(n<sup>[l]</sup>,m),
A<sup>[0]</sup>=X :(n<sup>[l]</sup>,m)

在你做深度神经网络的反向传播时，一定要确认所有的矩阵维数前后是一致的，可以大大提高代码的通过率。

---
#### 为什么用深层表示
我们都知道神经网络能处理很多问题，而且效果显著。其强大能力主要源自神经网络足够“深”，也就是说网络层数越多，神经网络就更加复杂和深入，学习也更加准确。接下来，我们从几个例子入手，看一下为什么深度网络能够如此强大。

先来看人脸识别的例子，如下图所示。经过训练，神经网络第一层所做的事就是从原始图片中提取出人脸的轮廓与边缘，即边缘检测。这样每个神经元得到的是一些边缘信息。神经网络第二层所做的事情就是将前一层的边缘进行组合，组合成人脸一些局部特征，比如眼睛、鼻子、嘴巴等。再往后面，就将这些局部特征组合起来，融合成人脸的模样。可以看出，随着层数由浅到深，神经网络提取的特征也是从边缘到局部特征到整体，由简单到复杂。可见，如果隐藏层足够多，那么能够提取的特征就越丰富、越复杂，模型的准确率就会越高。

语音识别模型也是这个道理。浅层的神经元能够检测一些简单的音调，然后较深的神经元能够检测出基本的音素，更深的神经元就能够检测出单词信息。如果网络够深，还能对短语、句子进行检测。记住一点，神经网络从左到右，神经元提取的特征从简单到复杂。特征复杂度与神经网络层数成正相关。特征越来越复杂，功能也越来越强大。

![image](https://pic1.zhimg.com/80/v2-2c245aac94020ea2bf466faaab40e156_hd.jpg)

比较下来，处理同一逻辑问题，深层网络所需的神经元个数比浅层网络要少很多。这也是深层神经网络的优点之一。

尽管深度学习有着非常显著的优势，Andrew还是建议对实际问题进行建模时，尽量先选择层数少的神经网络模型，这也符合奥卡姆剃刀定律（Occam’s Razor）。对于比较复杂的问题，再使用较深的神经网络模型。

---
#### 搭建神经网络块
我们选择神经网络其中一层，从这一层的计算着手，在第l层，你有参数W<sup>[l]</sup>和b<sup>[l]</sup>，正向传播里有输入的激活函数，输入是前一层a<sup>[l-1]</sup>，输出是a<sup>[l]</sup>，我们之前讲过z<sup>[l]</sup>=W<sup>[l]</sup>a<sup>[l-1]</sup>+b<sup>[l]</sup>,a<sup>[l]</sup>=g<sup>[l]</sup>(z<sup>[l]</sup>)，那么这就是你如何从输入a<sup>[l-1]</sup>走到输出a<sup>[l]</sup>。之后，你就可以把z<sup>[l]</sup>的值缓存起来，我在这里也会把包括这些缓存起来，因为缓存的z<sup>[l]</sup>对以后的正反向传播的步骤非常有效。

然后是反向步骤或者说反向传播步骤，同样也是第l层的计算，你会需要实现一个函数输入为da<sup>[l]</sup>，输出为da<sup>[l-1]</sup>的函数。一个小细节需要注意，输入在这里其实是da<sup>[l]</sup>以及所缓存的z<sup>[l]</sup>的值，之前计算好的z<sup>[l]</sup>值，除了输出da<sup>[l-1]</sup>的值以外，也需要输出你需要的梯度dW<sup>[l]</sup>和db<sup>[l]</sup>，这是为了实现梯度下降学习。

这就是基本的正向步骤的结构，我们把它称为正向函数，类似的在反向步骤中也会称为反向函数。总结起来就是，在l层，你会有正向函数，输入a<sup>[l-1]</sup>并且输出a<sup>[l]</sup>，为了计算结果你需要用W<sup>[l]</sup>和b<sup>[l]</sup>，以及输出到缓存的z<sup>[l]</sup>。然后用作反向传播的反向函数，是另一个函数，输入da<sup>[l]</sup>，输出da<sup>[l-1]</sup>，你就会得到对激活函数的导数，也就是希望的导数值da<sup>[l]</sup>。a<sup>[l-1]</sup>是会变的，前一层算出的激活导数。在这个方块里你需要W<sup>[l]</sup>和b<sup>[l]</sup>，最后你要计算的是dz<sup>[l]</sup>。这个反向函数可以计算输出dW<sup>[l]</sup>和db<sup>[l]</sup>

![image](https://pic2.zhimg.com/v2-dad16d5de3c10166df1cc0c112f3dfb2_r.jpg)

如果实现了这两个函数(正向和反向)，然后神经网络的计算过程会是这样的。

![image](https://pic4.zhimg.com/v2-45cadf1acbaf009556f5aee163f5f85c_r.jpg)

把输入特征a<sup>[0]</sup>，放入第一层并计算第一层的激活函数，用a<sup>[1]</sup>表示，你需要W<sup>[1]</sup>和b<sup>[1]</sup>来计算，之后也缓存z<sup>[1]</sup>的值。之后喂到第二层，第二层里，需要用到W<sup>[2]</sup>和b<sup>[2]</sup>，你会计算第二层的激活函数a<sup>[2]</sup>。后面几层依次类推，直到最后你计算出了a<sup>[L]</sup>。在这些过程里，我们缓存了所有的z值，这就是正向传播的步骤。

对反向传播的步骤而言，我们需要一系列的反向迭代，就是这样的反向梯度，你需要把da<sup>[l]</sup>的值放在这里，然后这个方块会给我们da<sup>[L-1]</sup>的值，依次类推，直到我们得到da<sup>[2]</sup>和da<sup>[1]</sup>，你还可以计算多一个输入值，就是da<sup>[0]</sup>，但这其实就是你的输入特征导数，并不重要。起码对于训练监督学习的权重不重要，你可以止步于此。反向传播步骤中也会输出dW<sup>[l]</sup>和db<sup>[l]</sup>。

神经网络的一步训练包含了，从a<sup>[0]</sup>开始，也就是 𝑥然后经过一系列正向传播计算得到，之后再用输出值计算这个（第二行最后方块），再实现反向传播。现在你就有所有的导数项了，𝑊也会在每一层被更新为𝑊= 𝑊−𝛼 𝑑𝑊，𝑏也一样，𝑏= 𝑏−𝛼𝑑𝑏，反向传播就都计算完毕，我们有所有的导数值，那么这是神经网络一个梯度下降循环。

---
#### 前向和反向传播
我们继续接着上一部分的流程块图的内容，推导神经网络正向传播过程和反向传播过程的具体表达式。
首先是正向传播的过程，令层数为l层，输入是a<sup>[l-1]</sup>，输出是a<sup>[l]</sup>，缓存变量是z<sup>[l]</sup>
```math
z^{[l]}=W^{[l]}a^{[l-1]}+b^{[l]}

a^{[l]}=g^{[l]}(z^{[l]})
```
m个训练样本，向量化形式为：

```math
Z^{[l]}=W^{[l]}A^{[l-1]}+b^{[l]}

A^{[l]}=g^{[l]}(Z^{[l]})
```
然后是反向传播过程，输入时da<sup>[l]</sup>，输出是da<sup>[l-1]</sup>,dW<sup>[l]</sup>，db<sup>[l]</sup>。其表达式如下：

```math
dz^{[l]}=da^{[l]}*g^{[l]'}(z^{[l]})

dW^{[l]}=dz^{[l]}·a^{[l-1]}

db^{[l]}=dz^{[l]}

da^{[l-1]}=W^{[l]T}·dz^{[l]}
```
由上述第4个表达式可得

```math
da^{[l]}=W^{[l+1]T}·dz^{[l+1]}
```
代入到第一个表达式中得：

```math
dz^{[l]}=W^{[l+1]T}·dz^{[l+1]}*g^{[l]'}(z^{[l]})
```
该式非常重要，反映了dz<sup>[l+1]</sup>与dz<sup>[l]</sup>的递推关系。

m个训练样本，向量化形式为:

```math
dZ^{[l]}=dA^{[l]}*g^{[l]'}(Z^{[l]})

dW^{[l]}={1\over m}dZ^{[l]}·A^{[l-1]T}

db^{[l]}={1\over m}np.sum(dZ^{[l]},axis=1,keepddim=True)

dA^{[l-1]}=W^{[l]T}·dZ^{[l]}

dZ^{[l]}=W^{[l+1]T}·dZ^{[l+1]}*g^{[l]'}(Z^{[l]})

```

---
#### 参数 VS 超参数
想要你的深度神经网络起到很好的效果，你还需要规划好你的参数以及超参数。

什么是超参数？

比如算法中的learning rate &alpha;(学习率)、iterations(梯度下降法循环的数量)、L(隐藏层数目)、n<sup>[l]</sup>(隐藏层单元数目)、choice of activation function(激活函数的选择)都需要由你来设置，这些数字实际上控制了最后的参数W和b的值，所有它们被称作超参数。

如何设置最优的超参数是一个比较困难的、需要经验知识的问题。通常的做法是选择超参数一定范围内的值，分别代入神经网络进行训练，测试cost function随着迭代次数增加的变化，根据结果选择cost function最小时对应的超参数值。这类似于validation的方法。

---
#### 深度学习和大脑有什么关联
关联不大。
