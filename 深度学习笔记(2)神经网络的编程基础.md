#### 二分类
当实现一个神经网络的时候，我们需要知道一些非常重要的技术和技巧。例如在一个包含有m个样本的训练集，我们很可能习惯于用一个for循环来遍历所有的样本，但是当实现神经网络的时候，我们通常不直接使用for循环来遍历整个的数据集。

神经网络的训练过程分为前向传播和后向传播两个独立的部分。

符号定义：
x:表示一个n<sub>x</sub>维数据。维度为(n<sub>x</sub>,1)

y:表示输出结果，取值为(0,1)

(x<sup>(i)</sup>,y<sup>(i)</sup>):表示第i组数据，可能是训练数据，也可能是测试数据，此处默认为训练数据。

X=[x<sup>(1)</sup>,x<sup>(2)</sup>,...x<sup>(m)</sup>]:表示所有的训练数据集的输入值，放在一个n<sub>x</sub>*m的矩阵中，其中m表示样本的数目。

Y=[y<sup>(1)</sup>,y<sup>(2)</sup>,...,y<sup>(m)</sup>]:对应所有的训练数据集的输入值，维度为1*m。

我们会定义一个矩阵用大写𝑋的表示，它由输
入向量x<sup>(1)</sup>,x<sup>(2)</sup>等组成，如下图放在矩阵的列中，所以现在我们把x<sup>(1)</sup>)作为第一列放在矩阵中，x<sup>(2)</sup>)作为第二列，x<sup>(m)</sup>到第𝑚列，然后我们就得到了训练集矩阵X。所以这个矩阵有𝑚列，𝑚是训练集的样本数量，然后这个矩阵的高度记为n<sub>x</sub>，注意有时候可能因为其他某些原因，矩阵X会由训练样本按照行堆叠起来而不是列，x<sup>(1)</sup>的转置直到x<sup>(m)</sup>)的转置，但是在实现神经网络的时候，使用左边的这种形式，会让整个实现的过程变得更加简单。

---
#### 对数几率回归
对于一个二元分类来讲，给定一个输入特征向量X，它可能对应一张图片，你想识别这张图片看它是否是一只猫或者不是一只猫。你想要算法能够输出你的预测，你只能称之为y的估计。在之前提过，X是一个n<sub>x</sub>维的向量(相当于有n<sub>x</sub>个特征的特征向量)。我们用&omega;来表示对数几率回归的参数，这也是一个n<sub>x</sub>维向量(因为&omega;实际上是特征权重，维度与特征向量相同)，参数里面还有b，这是一个实数(表示偏差)。所以给出输入x以及参数&omega;和b之后，我们怎样输出一个预测值.

```math
\hat y=\omega^Tx+b
```
这时候我们得到的是一个关于输入x的线性函数，实际上是你在做线性回归用到的，但是这对于一个二元分类不是一个好的算法。

在对数几率回归中，我们可以将线性函数转化为非线性函数。下面是sigmoid函数的图像。

![image](https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/320px-Logistic-curve.svg.png)


```math
g(z)={1\over{1+e^{-z}}}
```
这里要说明的是如果z非常大，那么e<sup>-z</sup>将会接近于0，关于z的sigmoid函数函数，将会近似等于1除以1加上某个非常接近于0的项，因为e的指数如果绝对值很大的负数的话，这项将会接近于0，因为e的指数是个绝对值很大的负数的话，这项将接近于1。反之，则接近于0。

我们对神经网络编程的时候经常会让参数&omega;和参数b分开。在这里b对应的是一种偏置。在之前的机器学习得课程里，可能接触过其他的表示方式。

---
#### 代数几率回归的代价函数
为了训练参数&omega;和参数b，我们需要代价函数，通过训练代价函数得到参数&omega;和参数b。

**损失函数：**

损失函数，又叫做误差函数，用来衡量算法的运行情况，LossFunction:

我们通过这个L称为的损失函数，来衡量预测输出值和实际值有多接近。一般我们用预测值和实际值的平方差或者平方差的一半，但是在对数几率回归中，我们不这么做，因为我们在学习对数几率回归参数的时候，会发现我们的优化目标不是凸优化，只能找到多个局部最优值，梯度下降法很可能找不到全局最优值，虽然平方差是一个不错的损失函数，但是，我们在对数几率回归模型中会定义另外一个损失函数。

```math
L(\hat y,y)=-ylog(\hat y)-(1-y)log(1-\hat y)
```
损失函数是在单个训练样本中定义的，它衡量的是算法在单个训练样本中表现如何。为了衡量算法在全部训练样本中的表现如何，我们需要定义一个算法的代价函数，算法的代价函数，是m个样本的损失函数求和在除以m:

```math
J(\omega,b)={1\over m}\sum_{i=1}^m\left(-y^{(i)}log\hat y^{(i)}-(1-y^{(i)})log(1-\hat y^{(i)})\right)
```
损失函数只适用于像这样的单个训练样本，而代价函数是参数的总代价，所以在训练对数几率回归模型时，我们需要找到合适的&omega;和b，来让代价函数J的总代价降到最低。根据我们对对数几率回归算法的推导及对单个样本损失函数的推导和针对算法所选用的参数的总代价函数的推导，结果表明对数几率回归可以看做是一个非常小的神经网络。

---
#### 梯度下降法
梯度下降法可以来做什么？

在测试集上通过最小化代价函数(成本函数)J(&omega;b)来训练的参数&omega;和b。

具体见机器学习笔记(3)、(4)

#### 导数
#### 计算图的导数计算

---
#### 对数几率回归中的梯度下降法
本节讨论通过计算偏导数来实现对数几率回归的梯度下降法，它的关键点是几个重要的公式，器作用是用来实现对数几率回归中的梯度下降法。

假设样本只有2个特征x<sub>1</sub>和x<sub>2</sub>，为了计算z，我们需要参数&omega;<sub>1</sub>、&omega;<sub>2</sub>和b，除此之外还有特征值x<sub>1</sub>和x<sub>2</sub>。因此z的计算公式为：

```math
z=\omega_1x_1+\omega_2x_2+b
```
回想一下对数几率回归的公式定义如下：

```math
\hat y=a=\sigma(z)
```
其中

```math
z=\omega^Tx+b

\sigma(z)={1\over{1+e^{-z}}}
```
损失函数：

```math
L(\hat y^{(i)},y^{(i)})=-y^{(i)}log\hat y^{(i)}-(1-y^{(i)})log(1-\hat y^{(i)} )
```
代价函数：

```math
J(\omega,b)={1\over m}\sum_{i=1}^mL(\hat y^{(i)},y^{(i)})
```
现在只考虑单个样本的情况，单个样本的代价函数定义如下：

```math
L(a,y)=-(ylog(a))+(1-y)log(1-a)
```
其中a是对数几率回归的输出，y是样本的标签值，现在我们可以画出表示这个计算的计算图，这里先复习一下梯度下降法。&omega;和b的修正量可以表达如下：

```math
\omega:=\omega-\alpha {∂J(\omega,b)\over ∂\omega}

b:=b-\alpha{∂J(\omega,b)\over ∂b}
```
通过微积分得到：

```math
{dL(a,y)\over da}={-y\over a}+{(1-y)\over (1-a)}
```

```math
{dL(a,y)\over dz}={dL(a,y)\over da}·{da \over dz}={-y\over a}+{(1-y)\over (1-a)}·a(1-a)=a-y
```
现在进行最后一步反向推导

```math
{∂L\over ∂\omega_1}=x_1dz=(a-y)x_1

{∂L\over ∂b}=(a-y)
```

因此对于单个样本的梯度下降法，你需要做的就是上面所述的那些。

---
#### m个样本的梯度下降法
这里面有很多的细节，但让我们把这些装进一个具体的算法。同时你需要一起应用的就是逻辑回归和梯度下降。

我们初始化J=0,d&omega;<sub>1</sub>=0;d&omega;<sub>2</sub>=0;db=0;

代码流程：

```math
J=0;d\omega_1=0;d\omega_2=0;db=0

for\text\ i=1 \text\ to\text\ m 

    z^{(i)}=\omega^Tx^{(i)}+b
    
    a^{(i)}=\sigma\left(z^{(i)}\right)
    
    J+=-\left[y^{(i)}loga^{(i)}+(1-y^{(i)})(1-loga^{(i)})  \right];
    
    dz^{(i)}=a^{(i)}-y^{(i)};
    
    d\omega_1 +=x_1^{(i)}dz^{(i)};
    
    d\omega_2 +=x_2^{(i)}dz^{(i)};
    
    db +=dz^{(i)};
    
J/=m;

d\omega_1/=m;

d\omega_2/=m;    

db/=m;

\omega=\omega-\alpha*d\omega;

b=b-\alpha*db;
```
幻灯片上只应用了一步梯度下降。因此你需要重复以上内容很多次，以应用多次梯度下
降。看起来这些细节似乎很复杂，但目前不要担心太多。

但这种计算中有两个缺点，也就是说应用此方法在逻辑回归上你需要编写两个for循环。
第一个 for 循环是一个小循环遍历𝑚个训练样本，第二个 for 循环是一个遍历所有特征的 for
循环。这个例子中我们只有 2 个特征，所以𝑛等于 2 并且𝑛𝑥 等于 2。 但如果你有更多特征，
你开始编写你的因此𝑑𝑤1，𝑑𝑤2，你有相似的计算从𝑑𝑤3一直下去到𝑑𝑤𝑛。所以看来你需要一
个 for 循环遍历所有𝑛个特征。 

当你应用深度学习算法，你会发现在代码中显式地使用for循环使你的算法很低效，同时在深度学习领域会有越来越大的数据集。所以能够应用你的算法且没有显示的for循环会是重要的，并且会帮助你适用更大的数据集。所以这里有一些叫做向量化的技术，它可以允许你的代码摆脱这些显示的for循环。

---
#### 向量化
向量化是非常基础的去除代码中for循环的艺术，在深度学习安全领域深度学习实践中，你会经常发现自己训练大数据集，因为深度学习处理大数据集效果很棒，所以你的运行速度很重要。

#### 向量化的更多例子

#### 向量化对数几率回归
首先让我们回顾一下对数几率回归的前向传播的步骤。如果你有m个训练样本，然后对第一个样本进行预测，你需要这样计算，计算z，我正在使用这个熟悉的公式
```math
z^{(1)}=\omega^Tx^{(1)}+b
```
然后计算激活函数：

```math
a^{(1)}=\sigma(z^{(1)})
```
计算第一个样本的预测值y。

依次类推，如果你有m个训练样本，你可能需要这样做m次。可看出，为了完成前向传播步骤，即对我们的m个样本都计算出预测值，有一个办法可以并且不需要任何明确的for循环。

我们曾定义了一个矩阵X作为你的训练输入，像这样不同的列中堆积在一起。这是一个n<sub>x</sub>行m列的矩阵。我现在将它写为Python中numpy的形式(n<sub>x</sub>,m)

```math
X=\begin{bmatrix}|&&|&& &&|\\
x^{(1)}&&x^{(2)}&&\cdots&&x^{(m)}\\
|&&|&& &&|
\end{bmatrix}\in R^{n_x*m}
```

```math
Z=\begin{bmatrix}z^{(1)}&&z^{(2)}&&\cdots&&z^{(m)}\end{bmatrix}
=\omega^TX+\begin{bmatrix}b&&b&&\cdots&&b\end{bmatrix}=\begin{bmatrix}\omega^Tx^{(1)}b&&\omega^Tx^{(2)}b&&\cdots&&\omega^Tx^{(m)}b\end{bmatrix}

```

```math
A=\begin{bmatrix}a^{(1)}&&a^{(2)}&&\cdots&&a^{(m)}\end{bmatrix}=\sigma(Z)

```
首先如何在一个步骤中计算z<sup>(1)</sup>、z<sup>(2)</sup>、z<sup>(3)</sup>等等。实际上只用了一行代码。所以，我打算先构建一个1 * m的矩阵，实际上它就是一个行向量，同时我们准备计算z<sup>(1)</sup>、z<sup>(2)</sup>......一直到z<sup>(m)</sup>，所有的值都是在同一时间内完成，结果发现它可以表示为&omega;的转置乘以X，然后再加上向量[b b ...b]。[b b ...b]是一个1 * m的向量或者1 * m的矩阵或者是一个m维的行向量。

如果你参照上面的定义，第一个元素恰好是z<sup>(1)</sup>的定义。第二个元素恰好是z<sup>(2)</sup>的定义。所以，因为X是一次获得的，当你看到你的训练样本，一个一个横向堆积起来。你就得到了Z。

numpy 命令是𝑍 = 𝑛𝑝.𝑑𝑜𝑡(𝑤.𝑇,𝑋) + 𝑏。这里在 Python 中有一个巧妙的地方，这里 𝑏 是一个实数，或者你可以说是一个 1 × 1 矩阵，只是一个普通的实数。但是当你将这个向量加上这个实数时，Python 自动把这个实数 𝑏 扩展成一个 1 × 𝑚 的行向量。所以这种情况下的操作似乎有点不可思议，它在 Python 中被称作广播(brosdcasting)。

概括一下，你刚刚看到如何利用向量化在同一时间内高效地计算所有的激活函数的所有 𝑎值。接下来，可以证明，你也可以利用向量化高效地计算反向传播并以此来计算梯度。让我们在下一个视频中看该如何实现。

---
#### 向量化 对数几率回归的梯度输出
如何进行向量化，对整个训练集的预测结果a，这是我们之前已经讨论过的内容，如何同时计算m个数据，并且实现一个非常高效的对数几率回归算法。

之前，我们在讲梯度计算的时候，对m个训练数据做同样的运算，我们可以定义一个新的变量dZ=[dz<sup>(1)</sup>,dz<sup>2</sup>,...,dz<sup>m</sup>]，所有的dz变量横向排列，因此dZ是一个1*m的矩阵，或者说是一个m维行向量。我们已经知道如何计算A，即[a<sup>(1)</sup>,a<sup>(2)</sup>,...,a<sup>(m)</sup>]，我们需要找到这样一个行向量Y=[y<sup>(1)</sup>,y<sup>(2)</sup>,...,y<sup>(m)</sup>]。由此，我们可以这样计算dZ=A-Y=[a<sup>(1)</sup>-y<sup>(1)</sup>,a<sup>(2)</sup>-y<sup>(2)</sup>,...,a<sup>(m)</sup>-y<sup>(m)</sup>]，不难发现第一个元素就是dz<sup>(1)</sup>，第二个元素就是dz<sup>(2)</sup>.....所以我们仅仅需要一行代码，就可以同时完成这所有的计算。

首先，我们来看db，不难发现

```math
db={1\over m}\sum_{i=1}^mdz^{(i)}
```
我们知道所有的dz<sup>(i)</sup>已经组成一个行向量dZ，所以在Python中，我们很容易想出db=1/m *np.sum(dZ);

接下来看d&omega;，我们先写出它的公式

```math
d\omega={1\over m}*X*dz^T
```
其中，X是一个行向量，因此展开后

```math
d\omega={1\over m}*(x^{(1)}dz^{(1)}+x^{(2)}dz^{(2)}+...+x^{(m)}dz^{(m)})
```
因此，我们可以用两行代码进行计算：
这样就避免了在训练集上使用for循环。

我们的目标是不使用for循环，而是向量，我们可以这样做：

```math
Z=\omega^TX+b=np.dot(\omega.T,X)+b

A=\sigma(Z)

dZ=A-Y

d\omega={1\over m}*X*dZ^T

db={1\over m}*np.sum(dZ)

\omega:=\omega-\alpha*d\omega

b:=b-\alpha*db

```
现在我们利用前五个公式完成了前向和后向传播，也实现了对所有训练样本进行预测和求导，再利用后两个公式，梯度下降更新参数。我们的目的是不使用for循环，所以我们就通过一次迭代实现一次梯度下降。但如果你希望多次梯度下降，那么仍然需要for循环，放在最外层。不过我们还是觉得一次迭代就进行一次梯度下降，避免使用任何循环比较舒服一些。

---
#### Python中的广播
具体可见Python数据科学手册

Python 的特性允许你使用广播（broadcasting）功能，这是 Python 的 numpy程序语言库中最灵活的地方。而我认为这是程序语言的优点，也是缺点。优点的原因在于它们创造出语言的表达性，Python语言巨大的灵活性使得你仅仅通过一行代码就能做很多事情。但是这也是缺点，由于广播巨大的灵活性，有时候你对于广播的特点以及广播的工作原理这些细节不熟悉的话，你可能会产生很细微或者看起来很奇怪的bug。例如，如果你将一个列向量添加到一个行向量中，你会以为它报出维度不匹配或类型错误之类的错误，但是实际上你会得到一个行向量和列向量的求和。

为了演示 Python-numpy的一个容易被忽略的效果，特别是怎样在 Python-numpy中构造向量，让我来做一个快速示范。首先设置𝑎 = 𝑛𝑝.𝑟𝑎𝑛𝑑𝑜𝑚.𝑟𝑎𝑛𝑑𝑛(5)，这样会生成存储在数组 𝑎 中的 5 个高斯随机数变量。之后输出 𝑎，从屏幕上可以得知，此时 𝑎 的 shape（形状）是一个(5,)的结构。这在 Python 中被称作一个一维数组。它既不是一个行向量也不是一个列向量，这也导致它有一些不是很直观的效果。举个例子，如果我输出一个转置阵，最终结果它会和𝑎看起来一样，所以𝑎和𝑎的转置阵最终结果看起来一样。而如果我输出𝑎和𝑎的转置阵的内积，你可能会想：𝑎乘以𝑎的转置返回给你的可能会是一个矩阵。但是如果我这样做，你只会得到一个数。 

所以我建议当你编写神经网络时，不要在它的 shape 是(5,)还是(𝑛,)或者一维数组时使用数据结构。相反，如果你设置 𝑎为(5,1)，那么这就将置于 5 行 1列向量中。在先前的操作里 𝑎 和 𝑎 的转置看起来一样，而现在这样的 𝑎 变成一个新的 𝑎 的转置，并且它是一个行向量。请注意一个细微的差别，在这种数据结构中，当我们输出 𝑎 的转置时有两对方括号，而之前只有一对方括号，所以这就是 1 行 5 列的矩阵和一维数组的差别。 
 
 
 