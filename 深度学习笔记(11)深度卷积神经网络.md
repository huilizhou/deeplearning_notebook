#### 为什么要进行实例探究

我们讲了基本构建，比如卷积层、池化层以及全连接层这些组件。事实上，过去几年计算机视觉研究中的大量研究都集中在如何把这些基本构件组合起来，形成有效的卷积神经网络。最直观的方式之一就是去看一些案例，就像很多人通过看别人的代码来学习编程一样，通过研究别人构建有效组件的案例是个不错的办法。实际上在计算机视觉任务中表现良好的神经网络框架往往也适用于其它任务，也许你的任务也不例外。也就是说，如果有人已经训练或者计算出擅长识别猫、狗、人的神经网络或者神经网络框架，而你的计算机视觉识别任务是构建一个自动驾驶汽车，你完全可以借鉴别人的神经网络框架来解决自己的问题。

典型的CNN模型包括：
- LeNet-5
- AlexNet
- VGG

除了这些性能良好的CNN模型之外，我们还会介绍Residual NetWork(ResNet)。其特点是可以构建很深很深的神经网络(目前为止最深的好像有152层)。

另外还会介绍Iinception Neural NetWork。

---
#### 经典网络
LeNet-5模型是Yann LeCun教授于1998年提出来的，它是第一个成功应用于数字识别问题的卷积神经网络。在MNIST数据中，它的准确率达到大约99.2%。典型的LeNet-5结构包含CONV layer，POOL layer和FC layer，顺序一般是CONV layer->POOL layer->CONV layer->POOL layer->FC layer->FC layer->OUTPUT layer，即 &hat; y 。下图所示的是一个数字识别的LeNet-5的模型结构：

![image](https://pic4.zhimg.com/v2-2233a03d7f4a3ff648488484abb4958f_r.jpg)

该LeNet模型总共包含了大约6万个参数。值得一提的是，当时Yann LeCun提出的LeNet-5模型池化层使用的是average pool，而且各层激活函数一般是sigmoid和tanh。现在，我们可以根据需要做出改进，使用max pool和激活函数ReLU。

AlexNet模型是由Alex Krizhevsky、Ilya Sutskever和Geoffrey Hinton共同提出的，其结构如下所示：

![image](https://pic2.zhimg.com/v2-fa2ac5d135f2bc9d0ffb2303300c87b6_r.jpg)

AlexNet网络包含8个学习层：5个卷积层、3个全连接层，最后的输出为一个1000类的Softmax层。在写这篇论文的时候，GPU的处理速度比较慢，所有AlexNet采用了非常复杂的在两块GPU上进行训练。大致原理事，这些层分别拆分到两个不同的GPU上，同时还有专门的方法用于两个GPU交流。

AlexNet模型与LeNet-5模型类似，只是要复杂一些，总共包含了大约6千万个参数。同样可以根据实际情况使用激活函数ReLU。原作者还提到了一种优化技巧，叫做Local Response Normalization(LRN)。而在实际应用中，LRN的效果并不突出。

VGG-16模型更加复杂一些，一般情况下，其CONV layer 和POOL layer 设置如下：
- CONV =3x3 filters,s=1,same
- MAX-POOL=2x2,s=2

VGG-16结构如下所示：

![image](https://pic4.zhimg.com/v2-c675003ccdb88e79cffaa6497655e9bf_r.jpg)

顺便说一下，VGG-16的这个数字16，就是指在这个网络中包含有16个卷积层和全连接层。确实是一个很大的网络，VGG-16的参数多达1.38亿。即便以现在的标准来看都算是非常大的网络。但VGG-16 结构并不复杂，这点非常吸引人，而且这种网络很规整，都是几个卷积层后面跟着可以压缩图像大小的池化层，池化层缩小了图像的高度和宽度。同时卷积层的过滤器数量变化存在着一定的规律，由64翻倍到128，再到256和512.作者可能认为512已经足够大了，所以后面的层就不再翻倍了。无论如何
，每一步都进行翻倍，或者说每一组卷积层进行过滤翻倍操作，正是设计这种网络结构的另一个简单原则。这种相对一致的网络结构对研究者很有吸引力。而它的主要缺点是需要训练的特征数量非常巨大。

我最喜欢它的一点是，文中揭示了，随着网络的加深，图像的高度和宽度都在以一定的规律不断缩小，每次池化后刚好缩小一半，而通道数量在不断增加，而且刚好也是在每组卷积操作后增加一倍。也就是说，图像缩小的比例和通道数增加的比例是有规律的。从这个角度来看，这篇论文很吸引人。

---
#### 残差网络
我们知道，如果神经网络层数越多，网络越深，源于梯度消失和梯度爆炸的影响，整个模型难以训练成功。解决的方法之一是人为地让神经网络某些层跳过下一层神经元的连接，隔层相连，弱化每层的强联系。这种神经网络被称为Residual NetWork(ResNets)。

Residual NetWorks 由许多隔层相连的神经元子模块组成，我们称之为Residual block。单个Residual block的结构如下图所示：

![image](https://pic2.zhimg.com/80/v2-0ec96d238a9d2569e363c58e333f66f5_hd.jpg)

上面图中红色部分就是skip connection，直接建立a<sup>[l]</sup>与a<sup>[l+2]</sup>之间的隔层联系。相应的表达式如下：

```math
z^{[l+1]}=W^{[l+1]}a^{[l]}+b^{[l+1]}

a^{[l+1]}=g(z^{[l+1]})

z^{[l+2]}=W^{[l+2]}a^{[l+1]}+b^{[l+2]}

a^{[l+2]}=g(z^{[l+2]}+a^{[l]})
```
a<sup>[l]</sup>直接隔层与下一层的线性输出相连，与z<sup>[l+2]</sup>共同通过激活函数(ReLU)输出a<sup>[l+2]</sup>。

该模型由Kaiming He, Xiangyu Zhang, Shaoqing Ren和Jian Sun共同提出。由多个Residual block组成的神经网络就是Residual Network。实验表明，这种模型结构对于训练非常深的神经网络，效果很好。另外，为了便于区分，我们把非Residual Networks称为Plain Network。

![image](https://pic4.zhimg.com/v2-a2ca85413ce15e8936ef8808b6eefd02_r.jpg)

Residual Network 结构如上图所示。

与Plain Network 相比，Residual Network能够训练更深层的神经网络，有效避免产生发生梯度消失和梯度爆炸。从下面两张图的对比中可以看出，随着神经网络层数的增加，Plain Network实际性能会变差，training error 甚至会变大，然而，Residual Network的训练效果却很好，training error 一直呈下降趋势。

![image](https://pic1.zhimg.com/v2-1acee3ecadf5e2e82fb5912e3fe3e428_r.jpg)

---
#### 残差网络为什么有用
下面用个例子来解释为什么ResNet能够训练更深层的神经网络。

![image](https://pic1.zhimg.com/v2-82ffab3e338c5fbc23682525a115429f_r.jpg)

如上图所示，输入x经过很多层神经网络输入之后输出a<sup>[l]</sup>，a<sup>[l]</sup>经过一个Residual blocks 输出a<sup>[l+2]</sup>。a<sup>[l+2]</sup>的表达式为：

```math
a^{[l+2]}=g(z^{[l+2]}+a^{[l]})=g(W^{[L+2]}a^{[l+1]}+b^{[l+2]}+a^{[l]})
```
输入x经过Big NN后，若W<sup>[l+2]</sup>&approx;0,b<sup>[l+2]</sup>&approx;0,则有：

```math
a^{[l+2]}=g(a^{[l]})=ReLU(a^{[l]})=a^{[l]}. 
   when a^{[l]}\geq 0
```
可以看出，即使发生了梯度消失，W<sup>[l+2]</sup>&approx;0，b<sup>[l+2]</sup>&approx;0，也能直接建立a<sup>[l+2]</sup>与a<sup>[l]</sup>的线性关系，且a<sup>[l+2]</sup>=a<sup>[l]</sup>，这其实就是identity function。a<sup>[l]</sup>直接连到a<sup>[l+2]</sup>，从效果上说，相当于直接忽略了a<sup>[l]</sup>之后的两层神经层。这样看似很深的神经网络，其实由于许多Residual blocks的存在，弱化削减了某些神经层之间的联系，实现隔层线性传递，而不是一味追求非线性关系，模型本身也就能“容忍”更深层的神经网络了。而且从性能上说，这两层额外的Residual blocks也不会降低Big NN的性能。

当然，如果Residual blocks中，a<sup>[l]</sup>和a<sup>[l+2]</sup>的维度不同，通常可以引入矩阵W<sub>s</sub>,与a<sup>[l]</sup>相乘，使得W<sub>s</sub>*a<sup>[l]</sup>的维度与a<sup>[l+2]</sup>一致。参数矩阵W<sub>s</sub>有2种方法得到：一种是将W<sub>s</sub>作为学习参数，通过模型训练得到；另一是固定W<sub>s</sub>值(类似单位矩阵)，不需要训练了，W<sub>s</sub>与a<sup>[l]</sup>乘积仅仅使得a<sup>[l]</sup>截断或者补零。这两种方法都可行。

下图所示的是CNN中ResNet的结构：

![image](https://pic4.zhimg.com/80/v2-4912730f7672c15eaeafeeda7836e9b8_hd.jpg)

ResNet同类型层之间，例如CONV layrers，大多使用same类型，保持维度相同。如果是不同类型之间的连接，例如CONV layer与POOL layer之间，如果维度不同，则引入矩阵W<sub>s</sub>

---
#### 网络中的网络，以及1*1卷积
Min Lin, Qiang Chen等人提出了一种新的CNN结构，即1x1 Convolutions，也称Networks in Networks。这种结构的特点是滤波器算子filter的维度为1x1。对于单个filter，1x1的维度，意味着卷积操作等同于乘积操作。

![image](https://pic4.zhimg.com/v2-e1d4f48252726c3f41800d27c2e5493d_r.jpg)

那么对于多个filters，1*1 Convolutions的作用是将上类似全连接层的神经网络结构。效果等同于Plain Network中a<sup>[l]</sup>到a<sup>[l+1]</sup>的过程。这点还是比较好理解的。

![image](https://pic2.zhimg.com/v2-c2222adcf03f095b956c85a07890aabe_r.jpg)

这个 1×1×32 过滤器中的 32 个数字可以这样理解，一个神经元的输入是 32 个数字(输入图片中左下角位置 32 个通道中的数字)，即相同高度和宽度上某一切片上的 32 个数字，乘以32个权重(将过滤器中的32个数字理解为权重)，然后应用ReLU非线性函数，在这里输出相应的结果。

一般来说，如果过滤器不止一个，而是多个，就好像有多个输入单元，器输入内容为一个切片上所有数字，输出结果是6x6过滤器数量。

所以 1×1 卷积可以从根本上理解为对这 32 个不同的位置都应用一个全连接层，全连接
层的作用是输入32个数字（过滤器数量标记为n<sub>c</sub><sup>[l+1]</sup>，在这 36 个单元上重复此过程）,输出
结果是 6×6×#filters（过滤器数量），以便在输入层上实施一个非平凡（non-trivial）计算。

这种方法称为1x1卷积，有时也被称为Network in Network。虽然论文中关于架构的详细内容并没有得到广泛应用，但是 1×1 卷积或 Network in Network 这种理念却很有影响力，很多神经网络架构都受到它的影响，包括下节课要讲的 Inception 网络。

1x1 Convolutions可以用来缩减输入图片的通道数目。方法如下图所示：

![image](https://pic4.zhimg.com/v2-dc6623d85b966172f48e6c8376fb7086_r.jpg)

假设这是一个28x28x192的输入层，你可以使用池化层压缩它的高度和宽度，这个过程我们很清楚。但是如果通道数过大，该如何把它压缩为28x28x32维度的层呢？

你可以用32个大小为1x1的过滤器，严格来讲每个过滤器大小都是1x1x192维，因为过滤器中通道数量必须与输入层通道数量保持一致。但是你使用了32个过滤器，输出层为28x28x32，就是压缩通道数(n<sub>c</sub>)的方法，对于池化层我只是压缩了这些层的高度和宽度。

在之后我们看到在某些网络中 1×1 卷积是如何压缩通道数量并减少计算的。当然如果你想保持通道数 192 不变，这也是可行的，1×1 卷积只是添加了非线性函数，当然也可以让网络学习更复杂的函数，比如，我们再添加一层，其输入为 28×28×192，输出为 28×28×192。

1x1卷积层就是这样实现了一些重要的功能的(doing something pretty non-trivial)，它给神经网络添加了一个非线性函数，从而减少或保持输入层中的通道数不变，当然如果你原意，也可以增加通道层的数量。这会对之后构建Inception网络很有帮助。


---
#### 谷歌 Inception 网络简介
之前我们介绍的CNN单层的滤波算子filter尺寸是固定的，1x1或者3x3等。而Inception Network在单层网络上可以使用多个不同尺寸的filters，进行same convolutions，把各filter下得到的输出拼接起来。除此之外，还可以将CONV layer 与POOL layer 混合，同时实现各种效果，但是要注意使用same pool。

![image](https://pic1.zhimg.com/v2-f8cf3ba74cb331fe8ef28815cf5e659f_r.jpg)

nception Network由Christian Szegedy, Wei Liu等人提出。与其它只选择单一尺寸和功能的filter不同，Inception Network使用不同尺寸的filters并将CONV和POOL混合起来，将所有功能输出组合拼接，再由神经网络本身去学习参数并选择最好的模块。

Inception Network在提升性能的同时，会带来大量的计算问题。例如下面的例子：

![image](https://pic4.zhimg.com/80/v2-22b14f55dc27fad9cd2a14170d95c272_hd.jpg)

此CONV layer 需要计算的量为：（28x28x32）x（5x5x192）=120m,其中m表示百万单位。可以看出但这一层的计算量都是很大的。为此，我们可以引入1x1 Convolutions 来减少计算量，结构如下图所示：

![image](https://pic3.zhimg.com/v2-9ab1ab462457cb473799aa49c815b0e8_r.jpg)

通常我们把该1x1 Convolution称为“瓶颈层(bottleneck layer)”。引入bottleneck layer之后，总共需要的计算量为：（28x28x16）x（1x1x192)+(28x28x32)x(5x5x16)=12.4m。很明显地。虽然多引入了1x1 Convolution层，但是总共的计算量减少了近90%，效果还是非常明显的。由此可见，1x1 Convolutions还可以有效减少CONV layer 的计算量。

---
#### Inception 网络

上一节，我们使用1x1 Convolution 来减少Inception Network 计算量大的问题。引入1x1 Convolution 后的Inception moudle 如下图所示：

![image](https://pic4.zhimg.com/80/v2-6e75cdf2554cd68cfc8a43ff1212ab79_hd.jpg)

多个Inception modules 组成Inception Network，效果如下图所示：

![20171213211346970](DCA1E70A66F44CA384BE1E2B9A6B7342)

上述 Inception Network除了由许多Inception modules组成之外，值得一提的是网络中间隐藏层也可以作为输出层Softmax，有利于防止过拟合。

---
#### 使用开源方案


---
#### 迁移学习
深度学习非常强大的功能之一就是有时候你可以将已经训练好的模型的一部分知识(网络结构)直接应用到另一个类似的模型中去，比如我们已经训练好了一个猫类识别的神经网络模型，那么我们可以直接把该模型中的一部分网络结构应用到使用X光片预测疾病的模型中去。这种学校方法被称为迁移学习。

如果我们已经有一个训练好的神经网络，用来做图像识别。现在，我们想要构建另外一个通过X光片进行诊断的模型。迁移学习的做法是无需重新构建新的模型，而是利用之前的神经网络模型，只改变样本输入、输出以及输出层的权重系数W<sup>[L]</sup>,b<sup>[L]</sup>。也就是说对于新的样本(X,Y)，重新训练输出层权重系数W<sup>[L]</sup>,b<sup>[L]</sup>，而其他层所有的权重系数W<sup>[L]</sup>,b<sup>[L]</sup>保持不变。

![v2-e779613b9aa3eda686735f10e06f189c_hd](C705C2F092D04C3BA8D73B9AB98FD2C5)

迁移学习，重新训练权重系数，如果需要构建新模型的样本数量较少，那么可以像刚才所说的，只训练输出层权重系数W<sup>[L]</sup>，b<sup>[L]</sup>。保持其它层所有的权重系数W<sup>[L]</sup>，b<sup>[L]</sup>不变。这种做法相对来说比较简单。如果样本数量足够多，那么也可以只保留网络结构，重新训练所有层的权重系数。这种做法使得模型更加精确，因为毕竟样本对模型的影响最大。选择哪种方法通常由数据量决定。

顺便提一下，如果重新训练所有权重系数，初始W<sup>[L]</sup>，b<sup>[L]</sup>由之前的模型训练得到，这一过程称为pre-training。之后，不断调试、优化W<sup>[L]</sup>，b<sup>[L]</sup>的过程称为fine-tuning。pre-training和fine-tuning分别对应上图的黑色箭头和红色箭头。

迁移学习之所以能这么做的原因是，神经网络浅层部分能够检测出许多图片固有特征，例如图像边缘、曲线等。使用之前训练好的神经网络部分结果有助于我们更快更准确地提取X光片特征。二者处理的都是图片，而图片处理是有相同的地方，第一个训练好的神经网络已经帮我们实现如何提取图片有用特征了。 因此，即便是即将训练的第二个神经网络样本数目少，仍然可以根据第一个神经网络结构和权重系数得到健壮性好的模型。

迁移学习可以保留原神经网络的一部分，再添加新的网络层。具体问题，具体分析，可以去掉输出层后再增加额外一些神经层。

![image](https://pic4.zhimg.com/80/v2-31e8b81dd37340d1faf5d71440be2085_hd.jpg)

总体说来，迁移学习的应用场合主要包括三点：
- Task A and Task B have the same input x.
- You have a lot more data for Task A than Task B.
- Lower level features from A could be helpful learning B

---
#### 数据扩充
常用的Data Argumentation方法是对已有的样本集进行Mirroring 和Random Cropping。

![image](https://pic2.zhimg.com/v2-5a9e5be55ae0fa7a870227d161bc5518_r.jpg)

另一种Data Augmentation的方法是color shifting。color shifting就是对图片的RGB通道数值进行随意增加或者减少，改变图片色调。

![image](https://pic3.zhimg.com/80/v2-ac229d61284bd2aa532e63fdca2f8595_hd.jpg)

除了可以随意改变RGB通道的数值外，还可以更有针对性地对图片的RGB通道进行PCA color augmentation，也就是对图片颜色进行主成分分析，对主要通道颜色进行增加或减少，可以采用高斯扰动的做法。这样也能增加有效的样本数量。具体的PCA color augmentation 做法可以查阅AlexNet的相关论文。

最后提一下，在构建大型神经网络的时候，data augmentation和training可以由两个不同的线程来进行。

---
#### 计算机视觉现状
神经网络需要数据，不同的网络模型所需的数据量是不同的。Object dection，Image recognition，Speech recognition所需的数据量依次增加。一般来说，如果data较少，那么就需要更多的hand-engineering，对已有data进行处理，比如上一节介绍的data augmentation。模型算法也会相对要复杂一些。如果data很多，可以构建深层神经网络，不需要太多的hand-engineering，模型算法也就相对简单一些。

![image](https://pic1.zhimg.com/v2-de7a8a6c9f69cb4bf0256ce60b2a67c9_r.jpg)

值得一提的是hand-engineering 是一项非常重要也比较困难的工作。很多时候，hand-engineering对模型训练效果影响较很大，特别是数据量不多的时候。

在模型研究或者竞赛方面，有一些方法能够有助于提升神经网络模型的性能：
- Ensembling: Train several networks independently and average their outputs.
- Multi-crop at test time: Run classifier on multiple versions of test images and average results.

![image](https://pic1.zhimg.com/80/v2-e961d3b3a9f51e7b9b11a73152b87248_hd.jpg)

但是由于这种方法计算成本比较大，一般不适用于实际项目开发。

最后我们还要灵活使用开源代码：
- Use archittectures of networks published in the literature
- Use open source implementations if possible
- Use pretrained models and fine-tune on your dataset

