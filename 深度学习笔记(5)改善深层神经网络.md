### 深度学习得实用层面

#### 训练，验证，测试集
在配置训练、验证和测试数据集的过程中做出正确的决策会在很大程度上帮助大家创建高效的神经网络，训练神经网络的时候，我们需要做出很多决策，例如：

神经网络分多少层；每层有多少个隐藏单元；学习速率是多少；各层采用哪些激活函数。

![image](https://pic2.zhimg.com/v2-02996acc8a8224838ab117185def91b1_r.jpg)

创建新应用的过程中，我们不可能一开始就准确预测出这些信息和超参数。实际上，应用型机器学习是一个高度迭代的过程，通常在项目启动时，我们会先有一个初步想法，比如构建一个含有特定层数，隐藏单元数量或数据集个数等等的神经网络，然后编码，并尝试运行这些代码，通过运行和测试得到神经网络或这些配置信息的运行结果，你可能会根据输出结果重新完善自己的想法，改变策略，或者为找到更好的神经网络不断迭代更新自己的方案。

现如今，深度学习已经在自然语言处理，计算机视觉，语音识别以及结构化数据应用等众多领域取得巨大成功。结构化数据无所不包，从广告到网络搜索。其中网络搜索不仅包括网络搜索引擎，还包括购物网站，从所有根据搜索栏词条传输结果的网站。再到计算机安全，物流，比如判断司机去哪接送货，范围之广，不胜枚举。

目前为止，我觉得，对于很多应用系统，即使是经验丰富的深度学习行家也不太可能一开始就预设出最匹配的超级参数，所以说，应用深度学习是一个典型的迭代过程，需要多次循环往复，才能为应用程序找到一个称心的神经网络，因此循环该过程的效率是决定项目进展速度的一个关键因素，而创建高质量的训练数据集，验证集和测试集也有助于提高循环效率。 

一般地，我们将样本数据分成三个部分:Trian/Dev/Test sets。Train sets 用来训练你的算法模型；Dev sets 用来验证不同算法的表现情况，从中选择最好的算法模型；Test sets用来测试最好算法的表现，作算法的无偏估计。

在机器学习的小数据量时代，常见做法是将所有数据三七分，就是人们常说的70%验证集，30%测试集，如果没有明确设置验证集，也可以按照60%训练，20%验证和20%测试集来划分。这是前几年机器学习领域普遍认可的最好的实践方法。

但是在大数据时代，我们现有的数据量可能是百万级别的，那么验证集和测试集的比例会趋向变得更小。因为验证集的目的就是为了验证不同的算法，检验哪种算法更有效，我们可能不需要拿出20%的数据作为验证集。

总结一下，在机器学习中，我们通常将样本分成训练集、验证集和测试集三个部分，数据集的规模相对较小。适用传统的划分比例，数据集规模较大的，验证集和测试集要小于数据总量的20%或10%。

现代深度学习的另一个趋势是越来越多的人在训练和测试集分布不匹配的情况下进行训练，假设你要构建一个用户可以上传大量图片的应用程序，目的是找出并呈现所有猫咪图片，可能你的用户都是爱猫人士，训练集可能是从网上下载的猫咪图片，而验证集和测试集是用户在这个应用上上传的猫的图片，就是说，训练集可能是从网络上抓下来的图片。而验证集和测试集是用户上传的图片。结果许多网页上的猫咪图片分辨率很高，很专业，后期制作精良，而用户上传的照片可能是用手机随意拍摄的，像素低，比较模糊，这两类数据有所不同，针对这种情况，**根据经验，我建议大家要确保验证集和测试集的数据来自同一分布，关于这个问题我也会多讲一些。因为你们要用验证集来评估不同的模型，尽可能地优化性能。如果验证集和测试集来自同一个分布就会很好**

由于深度学习算法需要大量的训练数据，为了获取更大规模的训练数据集，我们可以采用当前流行的各种创意策略。例如，网页抓取，代价就是训练数据集和测试数据集有可能不是来自同一分布。但只要遵循这个经验法则，你就会发现机器学习算法会变得更快。

最后一点，就算没有测试集也不要紧，测试集的目的是对最终所选定的神经网络系统做出无偏估计，如果不需要无偏估计，也可以不设置测试集。所以如果只有验证集，没有测试集，我们要做的就是，在训练集上训练，尝试不同的模型框架，在验证集上评估这些模型，然后迭代并选出适用的模型。因为验证集中已经涵盖测试集数据，其不再提供无偏性能评估。当然如果你不需要估计，那就再好不过了。

在机器学习过程中，如果只有一个训练集和一个验证集，而没有独立的测试集，遇到这种情况，训练集还被让人们称为训练集，而验证集则被称为测试集，不过在实际应用中，人们只是把测试集当成简单交叉验证集使用，并没有完全实现该术语的功能，因为他们把验证集数据过度拟合到测试集中去了。

如果某团队跟你说他们只设置了一个训练集和一个测试集，我会很谨慎，心想他们是不是真的有训练验证集，因为他们把验证集数据过度拟合到了测试集中，让这些团队改变叫法，改称其为“训练验证集”，而不是“训练测试集”，可能不太容易。即便我认为“训练验证集“在专业用词上更准确。实际上，如果你不需要无偏评估算法性能，那么这样是可以的。 

所以说，搭建训练验证集和测试集能够加速神经网络的集成，也可以更有效地衡量算法的偏差和方差，从而帮助我们更高效地选择合适方法来优化算法。

---
#### 偏差，方差
几乎所有机器学习从业人员都期望深刻理解偏差和方差，这两个概念易学难精，即使你自己认为已经理解了偏差和方差的基本概念，却总有一些意想不到的新东西出现。关于深度学习的误差问题，另一个趋势是对偏差和方差的权衡研究甚浅，你可能听说过这两个概念，但深度学习的误差很少权衡二者，我们总是分别考虑偏差和方差，却很少谈及偏差和方差的权衡问题，下面我们来一探究竟。 
 
 ![image](https://pic4.zhimg.com/v2-8da3571c3070a70c9ae57e22a12dd37c_r.jpg)

在上面图片所示的数据集中，如果给这个数据集拟合一条直线，可能得到一个对数几率回归拟合，但它并不能很好的拟合该数据，这是高偏差(high bias)的情况，我们称之为“欠拟合”(underfitting)。

相反的如果我们拟合一个非常复杂的分类器，比如深度神经网络或者含有隐藏单元的神经网络，可能就非常适用于这个数据集，但是看起来也不是一种很好的拟合方式，分类器方差较高(high variance)，数据过度拟合(overfitting)。

在这两者之间，存在这像图中一样，复杂程度适中，数据拟合度的分类器，这个数据拟合看起来更加合理，我们称之为“适度拟合”（just right)是介于过度拟合和欠拟合中间的一类。

在这样一个只有x<sup>1</sup>和x<sup>2</sup>两个特征的二维数据集中，我们可以绘制数据，将偏差和方差可视化。在多维空间数据中，绘制数据和可视化分割边界无法实现，但我们可以通过几个指标，来研究方差和偏差。

我们沿用猫咪图片分类这个例子，左边一张是猫咪图片，右边一张不是。理解偏差和方差的两个关键数据是训练集误差(Train set error)和验证集误差(Dev set error)，为了方便论证，假设我们可以辨别图片中的小猫，我们用肉眼识别几乎是不会出错的。

假定训练集误差是1%，为了方便论证，假定验证集误差是 11%，可以看出训练集设置得非常好，而验证集设置相对较差，我们可能过度拟合了训练集，在某种程度上，验证集并没有充分利用交叉验证集的作用，像这种情况，我们称之为“高方差”。 

假设训练集误差是 15%，我们把训练集误差写在首行，验证集误差是 16%，假设该案例中人的错误率几乎为 0%，人们浏览这些图片，分辨出是不是猫。算法并没有在训练集中得到很好训练，如果训练数据的拟合度不高，就是数据欠拟合，就可以说这种算法偏差比较高。相反，它对于验证集产生的结果却是合理的，验证集中的错误率只比训练集的多了 1%，所以这种算法偏差高，因为它甚至不能拟合训练集。

再举一个例子，训练集误差是 15%，偏差相当高，但是，验证集的评估结果更糟糕，错误率达到 30%，在这种情况下，我会认为这种算法偏差高，因为它在训练集上结果不理想，而且方差也很高，这是方差偏差都很糟糕的情况。 

再看最后一个例子，训练集误差是0.5%，验证集误差是 1%，用户看到这样的结果会很开心，猫咪分类器只有 1%的错误率，偏差和方差都很低。 

总结一下，我们讲了如何通过分析在训练集上训练算法产生的误差和验证集上验证算法产生的误差来诊断算法是否存在高偏差和高方差，是否两个值都高，或者两个值都不高，根据算法偏差和方差的具体情况决定接下来你要做的工作。

---
#### 机器学习基础
上节课我们讲通过训练集误差和验证集误差判断算法偏差或者方差是否偏高，帮助我们更加系统地在机器学习中运用这些方法来优化算法性能。

这是吴恩达老师讲的一些他在训练神经网络中用的基本方法。

初始模型训练完成后，我首先知道算法的偏差高不高，如果偏差较高，试着评估训练集或训练数据的性能。如果偏差的确很高，甚至无法拟合训练集，那么你要做的就是选择一个新的网络，比如含有更多隐藏层或者隐藏单元的网络，或者花费更多的时间来训练网络，或者尝试更为先进的优化算法，你也可以试试其他的方法，可能有用也可能没用。

采用更大规模的网络通常都会有所帮助，延长训练时间不一定会有用，但也没什么坏处。训练学习算法的时候，我会不断尝试这些方法，直至解决偏差问题，这是最低标准，反复尝试，直到可以拟合数据为止，至少能够拟合训练集。

如果网络足够大，通常可以很好的拟合训练集，只要你能扩大网络规模，如果图片很模糊，算法可能无法拟合该图片，但如果有人可以分辨出图片，如果你觉得基本误差不是很高，那么训练一个更大的网络，你就应该可以……至少可以很好地拟合训练集，至少可以拟合或者过拟合训练集。一旦偏差降低到可以接受的数值，检查一下方差有没有问题，为了评估方差，我们要查看验证集性能，我们能从一个性能理想的训练集推断出验证集的性能是否也理想，如果方差高，最好的解决办法就是采用更多数据，如果你能做到，会有一定的帮助，但有时候，我们无法获得更多数据，我们也可以尝试通过正则化来减少过拟合，这个我们下节课会讲。有时候我们不得不反复尝试，但是，如果能找到更合适的神经网络框架，有时它可能会一箭双雕，同时减少方差和偏差。如何实现呢？想系统地说出做法很难，总之就是不断重复尝试，直到找到一个低偏差，低方差的框架，这时你就成功了。

有两点需要大家注意： 
1. 高偏差和高方差是两种不同的情况，我们后续要尝试的方法也可能完全不同，我通常会用训练验证集来诊断算法是否存在偏差或者方差问题，然后根据结果选择尝试部分方法。举个例子，如果算法存在高偏差问题，准备更多的训练数据其实也没有什么用处，至少不是更有效的方法，所以大家要搞清楚存在的问题是偏差还是方差，还是两者都有问题，明确这一点有助于我们选择出最有效的算法。
2. 在机器学习初期阶段，关于所谓的偏差方差权衡的讨论屡见不鲜，原因是我们能够尝试的方法有很多。可以增加偏差，减少方差，也可以减少偏差，增加方差。但是在深度学习早期阶段，我们没有太多工具可以做到之减少偏差或方差而不影响到另一方。但在深度学习和大数据时代，只要持续训练一个更大的网络，只要准备更多的数据，那么也并非只有这两种情况，我们假定是这样，那么，只要正则适度，通常构建一个更大的网络便可以，在不影响方差的同时减少偏差，而采用更多数据通常可以在不过多影响偏差的同时减少方差。这两步实际要做的工作是：训练网络，选择网络或者准备更多的数据，现在我们有工具可以做到在减少偏差和方差的同时，不对另一方产生过多的不良影响。我觉得这是深度学习对监督学习大有裨益的一个重要原因，也是我们不用太多关注如何平衡偏差和方差的一个重要原因，但有时我们有很多选择减少偏差或方差而不增加另一方，最终，我们会得到一个非常规范化的网络。

---
#### 正则化
深度学习可能存在过拟合的问题————高方差，有两个解决方法，一个是正则化，另一个是准备更多的数据，这是个非常可靠的方法，但你可能无法时时刻刻准备足够多的训练数据或者获取更多数据的成本也很高，但正则化通常有助于避免过拟合或减少你的网络误差。

我们先来回顾一下，之前介绍的对数几率回归。采用L2 正则化，其表达式为：

```math
J(\omega,b)={1\over m}\sum_{i=1}^mL(\hat y^{(i)},y^{(i)})+{\lambda\over 2m}\begin{Vmatrix}\omega \end{Vmatrix}_2^{2}

\begin{Vmatrix}\omega\end{Vmatrix}_2^{2}=
\sum_{j=1}^{n_x}\omega_j^2=\omega^T\omega
```
为什么只正则化参数𝑤？为什么不再加上参数 𝑏  呢？你可以这么做，只是我习惯省略不写，因为𝑤通常是一个高维参数矢量，已经可以表达高偏差问题，𝑤可能包含有很多参数，我们不可能拟合所有参数，而𝑏只是单个数字，所以𝑤几乎涵盖所有参数，而不是𝑏，如果加了参数𝑏，其实也没太大影响，因为𝑏只是众多参数中的一个，所以我通常省略不计，如果你想加上这个参数，完全没问题。

L2正则化是最常见的正则化类型，你们可能听说过L1正则化；


```math
J(\omega,b)={1\over m}\sum_{i=1}^mL(\hat y^{(i)},y^{(i)})+{\lambda\over 2m}\begin{Vmatrix}\omega \end{Vmatrix}_1

\begin{Vmatrix}\omega\end{Vmatrix}_1=
\sum_{j=1}^{n_x}|{\omega_j}|
```
如果用的是L1正则化，&omega;最终会是稀疏的，也就是说&omega;向量有很多0，有人说这样有利于压缩模型，因为集合中参数均为0，存储模型占用的内存更小。实际上，虽然L1正则化使模型变得稀疏，却没有降低太多存储内存，所以我认为这不是L1正则化的目的，至少不是为了压缩模型。人们在训练网络的时候，越来越倾向于使用L2正则化。

L1、L2正则化中的&lambda;就是正则化参数(超参数的一种)。可以设置&lambda;设置不同的值，在Dev set 中进行验证。选择最佳的&lambda;。**顺便提一下在Python中，由于lambda是保留字，所以为了避免冲突，我们使用lambd来表示&lambda;。**以上就是在对数几率回归中实现正则化的过程，如何在神经网络中实现L2正则化呢？

神经网络含有一个成本函数，该函数包含W<sup>[1]</sup>,b<sup>[1]</sup>到W<sup>[l]</sup>,b<sup>[l]</sup>所有参数，字母L是神经网络所含的层数，因此成本函数等于m个训练样本损失函数的总和乘以1/m，正则化项为&lambda;/2m&sum;<sub>1</sub><sup>l</sup>|W<sup>[l]</sup>|<sup>2</sup>，我们称|W<sup>[l]</sup>|<sup>2</sup>为范数的平方，这个均值范数|W<sup>[l]</sup>|<sup>2</sup>(即平方范数)，被定义为矩阵中所有元素的平方求和。

```math
J(\omega^{[1]},b^{[1]},...,\omega^{[L]},b^{[L]})={1\over m}\sum_{i=1}^mL(\hat y^{(i)},y^{(i)})+{\lambda\over 2m}\sum_{l=1}^L\begin{Vmatrix}W^{[l]} \end{Vmatrix}^{2}

\begin{Vmatrix}W^{[l]} \end{Vmatrix}^{2}=\sum_{i=1}^{n^{[l]}}\sum_{j=1}^{n^{[l-1]}}(\omega_{ij}^{[l]})^2

```
我们看下求和公式的具体参数，第一个求和符号其值i从1到n<sup>[l]</sup>，第二个值j从1到n<sup>[l-1]</sup>，因为矩阵W是一个n<sup>[l]</sup>×n<sup>[l-1]</sup>的多维矩阵，n<sup>[l]</sup>表示l层的单元数量，n<sup>[l-1]</sup>表示l-1层的单元数量。

该矩阵范数被称作“弗罗贝尼乌斯范数”，用下标F标注，鉴于线性代数中一些神秘晦涩的原因，我们不称之为“矩阵L2范数”，而称它为“弗罗贝尼乌斯范数”，矩阵L2范数听起来更加自然一些，但鉴于一些大家无须知道的特殊原因，按照惯例，我们称它为“弗罗贝尼乌斯范数”，它表示一个矩阵中所有元素的平方和。

用backprop 计算出的dW的值，backprop会给出J对W的偏导数，实际上是W<sup>[l]</sup>，把W<sup>[l]</sup>替换为W<sup>[l]</sup>减去学习率乘以dW。这是我们没有增加正则化项之前。

```math
W^{[l]}=W^{[l]}- \alpha · dW^{[l]}
```
但是由于添加了正则项，dW<sup>[l]</sup>有个增量，在更新的时候，会多减去这个增量，使得W<sup>[l]</sup>比没有正则项的值要小一些。不断迭代更新，不断地减小。

```math
dW^{[l]}=dW_{before}^{[l]}+{\lambda \over m}W^{[l]}

W^{[l]}:=W^{[l]}-\alpha ·dW^{[l]}
=W^{[l]}-\alpha ·{(dW_{before}^{[l]}+{\lambda \over m}W^{[l]})}=(1-\alpha{\lambda\over m})W^{[l]}-\alpha · dW_{before}^{[l]}
```
其中：

```math
(1-\alpha{\lambda\over m})<1
```
因此L2范数正则化，也被称作“权重衰减”。之所以叫它“权重衰减”。是因为其权重指标乘以了一个小于1的系数。

以上就是在神经网络中应用L2正则化的过程。

---
#### 为什么正则化有利于预防过拟合
为什么正则化有利于防止过拟合呢？为什么它可以减少方差问题呢？我们通过例子直观体会一下。

![image](https://pic3.zhimg.com/v2-3c038099d50398a8eb77c1558b813c86_r.jpg)

左图是高偏差，右图是高方差，中间是Just Right。

现在我们来看一下这个庞大的深度拟合神经网络，我知道这张图不够大，深度也不不够，但是想象一下这是一个过拟合的神经网络，这是我们的代价函数J，含有参数W,b我们添加正则项，它可以避免数据权重矩阵过大，这就是弗罗贝尼乌斯范数，为什么压缩𝐿2范数，或者弗罗贝尼乌斯范数或者参数可以减少过拟合？ 

直观上理解就是如果正则化𝜆设置得足够大，权重矩阵𝑊被设置为接近于 0 的值，直观理解就是把多隐藏单元的权重设为 0，于是基本上消除了这些隐藏单元的许多影响。如果是这种情况，这个被大大简化了的神经网络会变成一个很小的网络，小到如同一个逻辑回归单元，可是深度却很大，它会使这个网络从过度拟合的状态更接近左图的高偏差状态。

但是𝜆会存在一个中间值，于是会有一个接近“Just Right”的中间状态。 

直观理解就是𝜆增加到足够大，𝑊会接近于 0，实际上是不会发生这种情况的，我们尝试消除或至少减少许多隐藏单元的影响，最终这个网络会变得更简单，这个神经网络越来越接近对数几率回归，我们直觉上认为大量隐藏单元被完全消除了，其实不然，实际上是该神经网络的所有隐藏单元依然存在，但是它们的影响变得更小了。神经网络变得更简单了，貌似这样更不容易发生过拟合，因此我不确定这个直觉经验是否有用，不过在编程中执行正则化时，你实际看到一些方差减少的结果。 
 
正则化为什么可以预防过拟合，假设我们用的是这样的双曲线激活函数。

![image](https://pic4.zhimg.com/v2-9ff54fdbfbdb21b7556e09dd91ac1d15_r.jpg)

用g(z)表示tanh(z)，那么我们发现，只要z非常小，如果z涉及少量参数，这里我们利用了双曲正切函数的线性状态。

如果正则化参数&lambda;很大，参数W会很小，相对来说，z也会很小。这个激活函数，也就是曲线函数tanh会相对呈线性状态，整个神经网络会计算离线性函数近的值，这个线性函数非常简单，并不是复杂的高度非线性函数，不会发生过拟合。

总结正则化，在增加正则化项时，应用之前定义的代价函数J，我们做过修改，增加了一项，目的是预防权重过大。

如果你使用梯度下降函数，在调试梯度下降的时，其中一步就是把代价函数J设计成这样一个函数，在调试梯度下降时，它代表梯度下降的调幅数量。可以看到，代价函数对于梯度下降的每个调幅都单调递减。如果你实施的是正则化函数，请牢记，J已经有一个全新的定义。如果你用的是原函数J，也就是这第一个项正则化项，你可能看不到单调递减现象。为了调试梯度下降，请务必使用新定义的J函数，它包含第二个正则化项。否则函数J可能不会在所有调幅范围内下降。

---
#### dropout 正则化
除了L2正则化，还有一个非常实用的正则化方法————“Dropout(随机失活)”，我们来看它的工作原理。

假设你在训练上图这样的神经网络，它存在过拟合，这就是dropout所要处理的，我们复制这个神经网络，dropout会遍历网络的每一层，并设置消除神经网络中节点的概率。假设网络中的每一层，每个节点都以抛硬币的方式设置概率，每个节点得以保留和消除的概率都是0.5，设置完节点概率，我们会消除一些节点，然后删除从该节点进出的连线，最后得到一个节点更少，规模更小的网络，然后用backprop方法进行训练。

![image](https://pic1.zhimg.com/v2-024bd8cd2bd6b48196e0ffe954f80071_r.jpg)

这是网络节点精简后的一个样本，对于其它样本，我们照旧以抛硬币的方式设置概率，保留一类节点集合，删除其他类型的节点集合。对于每个训练样本，我们都采用一个精简后神经网络来训练它，这种方法似乎有点怪，单纯遍历节点，编码也是随机的，可它真的有效，不过可想而知，我们针对每个训练样本训练规模极小的网络，最好你可能会认识到为什么要正则化网络，因为我们在训练极小的网络。

如何来实施dropout呢？方法有几种，最常用的一种方法是 inverted dropout(反向随机失活)，出于完整性考虑，我们用一个三层（𝑙=3）网络来举例说明。编码中会有很多涉及到的地方。我只举例说明如何在某一层中实施 dropout。 

首先要定义向量d，d<sup>[3]</sup>表示一个三层的dropout 向量：

```math
d3=np.random.rand(a3.shape[0],a3.shape[1])
```
然后看它是否小于某数，我们称之为keep-prob，keep-prop是一个具体数字，上个示例中是0.5，本例中是0.8，它表示保留某个隐藏单元的概率，此处keep-prob等于0.8，它意味着消除任意一个隐藏单元的概率是0.2，它的作用就是生成随机矩阵，如果对a<sup>[3]</sup>进行因子分解，效果也是一样的。d<sup>[3]</sup>是一个矩阵，每个样本和每个隐藏单元，其中d<sup>[3]</sup>中的对应值为1的概率都是0.8，对应为0的概率是0.2。

接下来要做的就是从第三层中获取激活函数，这里我们叫它a<sup>[3]</sup>，a<sup>[3]</sup>含有要计算的激活函数，a<sup>[3]</sup>等于上面的a<sup>[3]</sup>乘以d<sup>[3]</sup>，这里是元素相乘，它的作用就是让d<sup>[3]</sup>中所有等于0的元素(输出)，而各个元素等于0的概率只有20%，乘法运算最终把d<sup>[3]</sup>中所有等于0的元素于a<sup>[3]</sup>中相对的元素归0。

如果用 python实现该算法的话，d<sup>[3]</sup>则是一个布尔型数组，值为True和False，而不是1和0，乘法运算依然有效，Python会把True和False翻译为1和0。

最后，我们向外扩展a<sup>[3]</sup>，用它除以keep-prob参数。

下面我解释一下为什么要这么做，为方便起见，我们假设第三隐藏层上有50个单元或50个神经元，在一维上a<sup>[3]</sup>是50，我们通过因子分解将他拆分成50×m维的，保留和删除它们的概率分别为80%和20%，这意味着最后被删除或归零的单元平均有10(50x20%=10)个，现在我们来看下z<sup>[4]</sup>，z<sup>[4]</sup>=&omega;<sup>[4]</sup>a<sup>[3]</sup>+b<sup>[4]</sup>，我们的预期是a<sup>[3]</sup>减少20%，也就是说a<sup>[3]</sup>中有20%的元素被归零，为了不影响z<sup>[4]</sup>的期望值，我们需要用&omega;<sup>[4]</sup>a<sup>[4]</sup>/0.8，它将会修正或弥补我们所需的那20%，a<sup>[3]</sup>的期望值不会变。

它的功能是，不论 keep-prop 的值是多少 0.8，0.9 甚至是 1，如果 keep-prop 设置为 1，那么就不存在dropout，因为它会保留所有节点。反向随机失活（inverted dropout）方法通过除以 keep-prob，确保，a<sup>[3]</sup>的期望值不变。 

事实证明，在测试阶段，当我们评估一个神经网络时，用反向随机失活方法，使测试阶段变得更容易，因为它的数据扩展问题变少。

现在你使用的是𝑑向量，你会发现，不同的训练样本，清除不同的隐藏单元也不同。实际上，如果你通过相同训练集多次传递数据，每次训练数据的梯度不同，则随机对不同隐藏单元归零，有时却并非如此。比如，需要将相同隐藏单元归零，第一次迭代梯度下降时，把一些隐藏单元归零，第二次迭代梯度下降时，也就是第二次遍历训练集时，对不同类型的隐藏层单元归零。向量d或d<sup>[3]</sup>来决定第三层中哪些单元归零，无论用 foreprop 还是 backprop，这里我们只介绍了 foreprob。

显然在测试阶段，我们并未使用dropout，自然也就不用抛硬币来决定失活概率，以及要消除哪些隐藏单元了，因为在测试阶段进行预测时，我们不期望输出结果是随机的，如果测试阶段应用 dropout 函数，预测会受到干扰。理论上，你只需要多次运行预测处理过程，每一次，不同的隐藏单元会被随机归零，预测处理遍历它们，但计算效率低，得出的结果也几乎相同，与这个不同程序产生的结果极为相似。

---
#### 理解Dropout
Dropout可以随机删除网络中的神经元，他为什么可以通过正则化发挥如此大的作用呢？

直观上理解：不要依赖于任何一个特征，因为该单元的输入随时可能被清除，因此该单元通过这种方式传播下去，并为单元的四个输入增加一点权重，通过传播所有权重，dropout将产生收缩权重的平方范数的效果，和之前讲的𝐿2正则化类似；实施 dropout 的结果实它会压缩权重，并完成一些预防过拟合的外层正则化；𝐿2对不同权重的衰减是不同的，它取决于激活函数倍增的大小。 

总结一下，dropout的功能类似于𝐿2正则化，与𝐿2正则化不同的是应用方式不同会带来一点点小变化，甚至更适用于不同的输入范围。 

第二个直观认识是，我们从单个神经元入手，如图，这个单元的工作就是输入并生成一些有意义的输出。通过 dropout，该单元的输入几乎被消除，有时这两个单元会被删除，有时会删除其它单元。，它不能依靠任何特征，因为特征都有可能被随机清除，或者说该单元的输入也都可能被随机清除。我不愿意把所有赌注都放在一个节点上，不愿意给任何一个输入加上太多权重，因为它可能会被删除，因此该单元将通过这种方式积极地传播开，并为单元的四个输入增加一点权重，通过传播所有权重，dropout 将产生收缩权重的平方范数的效果，和我们之前讲过的𝐿2正则化类似，实施 dropout的结果是它会压缩权重，并完成一些预防过拟合的外层正则化。

事实证明，dropout被正式地作为一种正则化的替代形式，𝐿2对不同权重的衰减是不同的，它取决于倍增的激活函数的大小。 

总结一下，dropout的功能类似于𝐿2正则化，与𝐿2正则化不同的是，被应用的方式不同，dropout 也会有所不同，甚至更适用于不同的输入范围。 

使用dropout的时候，有几点需要注意。首先，不同隐藏层的dropout系数keep-prob可以不同。一般来说，神经元越多的隐藏层，keep-out可以设置得小一些，例如0.5；神经元越少的隐藏层，keep-out可以设置的大一些，例如0.8，设置是1。另外，实际应用中，不建议对输入层进行dropout，如果输入层维度很大，例如图片，那么可以设置dropout，但keep-ut应设置的大一些，例如0.8，0.9。总体来说，就是越容易出现overfitting的隐藏层，其keep-prob就设置的相对小一些。没有准确固定的做法，通常可以根据validation进行选择。

Dropout在电脑视觉CV领域应用比较广泛，因为输入层维度较大，而且没有足够多的样本数量。值得注意的是dropout是一种regularization技巧，用来防止过拟合的，最好只在需要regularization的时候使用dropout。

使用dropout的时候，可以通过绘制cost function来进行debug，看看dropout是否正确执行。一般做法是，将所有层的keep-prob全设置为1，再绘制cost function，即涵盖所有神经元，看J是否单调下降。下一次迭代训练时，再将keep-prob设置为其它值。

---
#### 其他正则化方法
除了𝐿2正则化和随机失活（dropout）正则化，还有几种方法可以减少神经网络中的过拟合: 
1. 数据扩展
 
假设你正在拟合猫咪图片分类器，如果你想通过扩增训练数据来解决过拟合，但扩增数据代价高，而且有时候我们无法扩增数据，但我们可以通过添加这类图片来增加训练集。例如，水平翻转图片，并把它添加到训练集。所以现在训练集中有原图，还有翻转后的这张图片，所以通过水平翻转图片，训练集则可以增大一倍，因为训练集有冗余，这虽然不如我们额外收集一组新图片那么好，但这样做节省了获取更多猫咪图片的花费。 

2. early stopping

还有另外一种常用的方法叫作 early stopping，运行梯度下降时，我们可以绘制训练误差，或只绘制代价函数𝐽的优化过程，在训练集上用 0-1 记录分类误差次数。

因为在训练过程中，我们希望训练误差，代价函数J都在下降，通过 early stopping，我们不但可以绘制上面这些内容，还可以绘制验证集误差，它可以是验证集上的分类误差，或验证集上的代价函数，逻辑损失和对数损失等，你会发现，验证集误差通常会先呈下降趋势，然后在某个节点处开始上升，early stopping 的作用是，你会说，神经网络已经在这个迭代过程中表现得很好了，我们在此停止训练吧，得到验证集误差。

术语 early stopping代表提早停止训练神经网络，训练神经网络时，我有时会用到 early stopping，但是它也有一个缺点。

主要缺点就是你不能独立地处理这两个问题，因为提早停止梯度下降，也就是停止了优化代价函数𝐽，因为现在你不再尝试降低代价函数𝐽，所以代价函数𝐽的值可能不够小，同时你又希望不出现过拟合，你没有采取不同的方式来解决这两个问题，而是用一种方法同时解决两个问题，这样做的结果是我要考虑的东西变得更复杂。 

如果不用 early stopping，另一种方法就是𝐿2正则化，训练神经网络的时间就可能很长。我发现，这导致超级参数搜索空间更容易分解，也更容易搜索，但是缺点在于，你必须尝试很多正则化参数𝜆的值，这也导致搜索大量𝜆值的计算代价太高。 

虽然𝐿2正则化有缺点，可还是有很多人愿意用它。吴恩达老师个人更倾向于使用𝐿2正则化，尝试许多不同的𝜆值，假设你可以负担大量计算的代价。而使用 early stopping 也能得到相似结果，还不用尝试这么多𝜆值。 

---
#### 正则化输入
训练神经网络，其中一个加速训练的方法就是归一化输入，假设一个训练集有两个特征，输入特征为2维，归一化需要两个步骤：

1. 零均值
2. 归一化方差


```math
\mu={1\over m}\sum_{i=1}^mX^{(i)}

\sigma^2={1\over m}\sum_{i=1}^m(X^{(i)})^2

X:={{X-\mu}\over \sigma^2}
```
以二维平面为例，下面展示其归一化的过程：

![image](https://pic1.zhimg.com/v2-fd3dd040742658863ee3b11ae4df8bea_r.jpg)

值得注意的是，由于训练集进行了标准化处理，那么测试集或在实际应用中，应该使用同样的&mu;和&sigma;<sup>2</sup>对其进行标准化处理。这样保证了训练集和测试集标准化操作一致。

之所以要对输入进行标准化操作，主要是为了让所有输入归一化同样的尺度上，方便进行梯度下降算法时能够更快更准确地找到全局最优解。假如输入特征是二维的，且x<sub>1</sub>的范围是[1,1000]，x<sub>2</sub>的范围是[0,1]。如果不进行标准化处理，x<sub>1</sub>与x<sub>2</sub>之间分布极不平衡，训练得到的w<sub>1</sub>和w<sub>2</sub>也会在数量级上差别很大。这样导致的结果是cost function与w和b的关系可能是一个非常细长的椭圆形碗。对其进行梯度下降算法时，由于w<sub>1</sub>和w<sub>2</sub>数值差异很大，只能选择很小的学习因子&alpha; ，来避免J发生振荡。一旦 &alpha; 较大，必然发生振荡，J不再单调下降。如下左图所示。

![image](https://pic3.zhimg.com/v2-6da0513b73860772bcb0541691cb54fd_r.jpg)

另外一种情况，如果输入特征之间的范围本来就比较靠近，那么不进行标准化操作也是没有太大影响的。但是，标准化处理在大多数场合下还是值得推荐的。

---
#### 梯度消失与梯度爆炸
训练神经网络，尤其是深度神经网络所面临的一个问题就是梯度消失或梯度爆炸，也就是你训练神经网络的时候，导数或梯度有时会变得非常大，或者非常小，甚至于以指数的方式变小，这加大了训练的难度。

举个例子来说明，假设一个多层的每层只包含两个神经元的深度神经网络模型，如下图所示：

![image](https://pic1.zhimg.com/v2-9dafeb4131ae3c1d2f7284642348cbf0_r.jpg)

为了简化复杂度，便于分析，我们令各层的激活函数为线性函数，即a<sup>[l]</sup>=g(z<sup>[l]</sup>)。而且忽略各层常数项b的影响，令b全部为0。那么，该网络的预测输出为

```math
\hat Y=W^{[L]}W^{[L-1]}W^{[L-2]}...W^{[3]}W^{[2]}W^{[1]}X
```
假设每个权重矩阵

```math
W^{[l]}=\begin{bmatrix}1.5&&0\\0&&1.5 \end{bmatrix}
```
从技术上来讲，最后一项有不同的维度，可能它就是余下的权重矩阵

```math
Y=W^{[1]}\begin{bmatrix}1.5&&0\\0&&1.5 \end{bmatrix}^{(L-1)}X
```
如果对于一个深度神经网络，L值较大，那么Y值也会非常大，实际上它呈指数级增长，它的增长比率是1.5<sup>L</sup>，因此对于一个深度神经网络，Y的值将爆炸式增长。

相反的，如果权重是0.5
```math
W^{[l]}=\begin{bmatrix}0.5&&0\\0&&0.5 \end{bmatrix}
```
从技术上来讲，最后一项有不同的维度，可能它就是余下的权重矩阵

```math
Y=W^{[1]}\begin{bmatrix}0.5&&0\\0&&0.5 \end{bmatrix}^{(L-1)}X
```
激活函数的值将呈指数级递减。

在神经网络中，激活函数将以指数级递减，虽然只是讨论了激活函数以及与L相关的指数级数增长或下降，它也适用与层数L相关的导数或梯度函数，也是呈指数级增长或指数递减。

对于当前的神经网络，假设𝐿 = 150，最近 Microsoft 对 152 层神经网络的研究取得了很大进展，在这样一个深度神经网络中，如果激活函数或梯度函数以与𝐿相关的指数增长或递减，它们的值将会变得极大或极小，从而导致训练难度上升，尤其是梯度指数小于L时，梯度下降算法的步长会非常非常小，梯度下降算法将花费很长时间来学习。 

我们讲了深度神经网络是如何产生梯度消失或爆炸问题，实际上，在很长一段时间内，它曾是训练神经网络的阻力，虽然有一个不能完全解决此问题的方案，但是已经在如何选择初始化权重问题上提供了很多帮助。

---
#### 神经网络的权重初始化
我们学习了深度学校网络如何产生梯度消失和梯度爆炸的问题，最终针对该问题，我们想出了一个不完整的解决方案，虽然不能彻底解决问题，却很有用，有助于我们为神经网络更谨慎地选择随机初始化参数。

我们先看只有一个神经元的情况，然后才是深度网络。

![image](https://pic1.zhimg.com/v2-d848ca4f12feb7343034e5e8c3f356dd_r.jpg)

深度神经网络模型中，以单个神经元为例，该层[l]的输入个数为n，其输出为：

```math
z=\omega_1x_1+\omega_2x_2+\omega_nx_n

a=g(z)
```
这里忽略了常数项b。为了让z不会过大或者过小，你可以看到n越大，你希望&omega;<sub>i</sub>越小，因为z是&omega;<sub>i</sub>x<sub>i</sub>的和，如果你把很多此类项相加，希望每项的值更小，最合理的方法就是设置&omega;<sub>i</sub>=1/n，n表示神经元的输入特征数量。
实际上你要做的就是

```
w[l]=np.random.randn(n[l],n[l-1])*np.sqrt(1/n[l-1])
```
如果是tanh激活函数就选用上面的方法。

如果你用的是ReLU激活函数。权重初始化一般令其方差为2/n：
```
w[l]=np.random.randn(n[l],n[l-1])*np.sqrt(2/n[l-1])
```
除此之外，Yoshua Bengio 提出了另外一种初始化&omega;的方法，令其方差为2/n<sup>[l-1]</sup>n<sup>[l]</sup>:
```
w[l]=np.random.randn(n[l],n[l-1])*np.sqrt(2/n[l-1]*n[l])
```
实际上，所有这些公式只是给你一个起点，它们给出初始化权重矩阵的方差的默认值，如果你想添加方差，方差参数则是另一个你需要调整的超级参数。，但我发现调优过程中产生的问题，虽然调优该参数能起到一定作用，但考虑到相比调优，其它超级参数的重要性，我通常把它的优先级放得比较低。 

至于选择哪种初始化方法因人而异，可以根据不同的激活函数选择不同方法。另外，我们可以对这些初始化方法中设置某些参数，作为超参数，通过验证集进行验证，得到最优参数，来优化神经网络。

---
#### 梯度的数值逼近
Back Propagation 神经网络有一项非常重要的测试是梯度检查(gradient checking)，其目的检查反向传播过程中梯度下降算法是否正确。该小节将先介绍如何近似求出梯度值。

![image](https://pic4.zhimg.com/v2-3cf7393501921c954665290e71688cf8_r.jpg)

利用微分的思想，函数f在&theta;处的梯度可以表示成：

```math
g(\theta)={{f(\theta+\varepsilon)-f(\theta-\varepsilon)}\over 2\varepsilon}
```
其中，&epsilon;>0，且足够小。

如果你对微积分和数值逼近有所了解，这些信息已经足够多了，重点是要记住，双边误差公式的结果更准确，下节课我们做梯度检验时就会用到这个方法。

---
#### 梯度检验
介绍完如何求出梯度的近似值之后，我们将介绍如何进行梯度检验，来验证训练过程中是否出现Bugs。

梯度检验首先要做的是分别将W<sup>[1]</sup>,b<sup>[1]</sup>,...,W<sup>[l]</sup>,b<sup>[l]</sup>这些矩阵构造成一维向量，然后将这些一维向量组合起来构成一个更大的一维向量&theta;。这样cost function J(W<sup>[1]</sup>,b<sup>[1]</sup>,...,W<sup>[l]</sup>,b<sup>[l]</sup>)就可以表示为J(&theta;)。

然后将反向传播过程中通过梯度下降法得到的dW<sup>[1]</sup>,db<sup>[1]</sup>,...,dW<sup>[l]</sup>,db<sup>[l]</sup>按照一样的顺序构造成一个一维向量d&theta;。d&theta;的维度与&theta;一致。

接着利用J(&theta;)对每个&theta;<sub>i</sub>计算近似梯度，其值与反向传播算法得到的d&theta;<sub>i</sub>相比较，检查是否一致。例如，对于第i个元素，近似梯度为：

```math
d\theta_{approx}[i]={{{J(\theta_1,\theta_2,...,\theta_{i+\varepsilon},...)}-{J(\theta_1,\theta_2,...,\theta_{i-\varepsilon},...)} }\over {2\varepsilon}}
```
计算完所有的&theta;<sub>i</sub>的近似梯度之后，可以计算d&theta;<sub>approx</sub>与d&theta;的欧式距离来比较二者的相似度。


```math
\begin{Vmatrix}
d\theta_{approx}-d\theta
\end{Vmatrix}_2 \over {\begin{Vmatrix}
d\theta_{approx}
\end{Vmatrix}_2 +\begin{Vmatrix}
d\theta
\end{Vmatrix}_2 }
```
一般来说，如果欧氏距离越小，例如 10<sup>-7</sup> ，甚至更小，则表明 d&theta;<sub>approx</sub> 与 d&theta; 越接近，即反向梯度计算是正确的，没有bugs。如果欧氏距离较大，例如 10<sup>-5</sup> ，则表明梯度计算可能出现问题，需要再次检查是否有bugs存在。如果欧氏距离很大，例如10<sup>-3</sup> ，甚至更大，则表明 d&theta;<sub>approx</sub> 与d&theta; 差别很大，梯度下降计算过程有bugs，需要仔细检查。

---
#### 梯度检验应用的注意事项
在进行梯度检查的过程中有几点需要注意的地方：

- 不要在整个训练过程中都进行梯度检查，仅仅作为debug使用。
- 如果梯度检查出现错误，找到对应出错的梯度，检查其推导是否出现错误。
- 注意不要忽略正则化项，计算近似梯度的时候要包括进去。
- 梯度检查时关闭dropout，检查完毕后再打开dropout。
- 随机初始化时运行梯度检查，经过一些训练后再进行梯度检查（不常用）。

